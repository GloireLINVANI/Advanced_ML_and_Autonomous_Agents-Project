{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjC5UydPYSvm"
   },
   "source": [
    "# Comparing Tree Search and Reinforcement Learning Approaches for King and Courtesan Game\n",
    "## Gloire LINVANI\n",
    "### CSC-52081-EP Advanced Machine Learning and Autonomous Agents Project\n",
    "\n",
    "### The following contains adapted material from the labs 6 and 7 developed by Jérémie Decock.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/refs/heads/main/assets/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC-52081-EP-2025](https://moodle.polytechnique.fr/course/view.php?id=19336)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0SUKVaBYuNt"
   },
   "source": [
    "# Colab Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAi4s3cPhZ4J"
   },
   "source": [
    "#### For practical reasons, we run the notebook in the Google Drive folder where it is located. We need to provide a path."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16114,
     "status": "ok",
     "timestamp": 1742263907729,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "uGI1hFYag0vn",
    "outputId": "61823615-e82b-4967-c147-489e40318b32"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Replace path_to_notebook with your actual path\n",
    "path_to_notebook = \"/content/drive/MyDrive/Colab Notebooks/M1 Data AI/Advanced Machine Learning and Autonomous Agents/Project\"\n",
    "\n",
    "%cd {path_to_notebook}\n",
    "\n",
    "!pwd\n",
    "!ls"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13873,
     "status": "ok",
     "timestamp": 1742263921603,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "4AixSpsoc0r3",
    "outputId": "b1e159b3-e8a1-4c1e-954c-4c62a51fa45a"
   },
   "source": [
    "# Installing Java\n",
    "\n",
    "!sudo apt update\n",
    "\n",
    "!sudo apt install default-jdk\n",
    "\n",
    "!java -version\n",
    "!javac -version"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeHdR2iIpEAF"
   },
   "source": [
    "### Starting Java servers: Port 3 for King and Courtesan server and port 9 for ID Alpha Beta agent server. You can change the ports if needed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5q3MfDbUePyA"
   },
   "source": [
    "# Starting the Java server in the background\n",
    "!nohup java -cp \"RL_KAC.jar:json-20250107.jar:commons-cli-1.4.jar\" games.kac.RL_Agents_KingAndCourtesanServer -p 3 > server1.log 2>&1 &"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KmdlBDpGpC3I"
   },
   "source": [
    "!nohup java -cp \"RL_KAC.jar:json-20250107.jar:commons-cli-1.4.jar\" games.kac.IDAlphaBetaServer -t 30 -p 9 > server2.log 2>&1 &"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5200,
     "status": "ok",
     "timestamp": 1742263927352,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "4fHJIc11ePyA",
    "outputId": "4271d8c6-1b8e-4f9e-b484-35dc8cb5c7ec"
   },
   "source": [
    "# Giving the server time to start\n",
    "import time\n",
    "\n",
    "print(\"Waiting for server to start...\")\n",
    "time.sleep(5)\n",
    "!cat server1.log  # Server log\n",
    "!cat server2.log"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTBH1rF1YSvp"
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLeNOCbkYSvq"
   },
   "source": [
    "This notebook relies on several libraries like `gymnasium`, `torch`, `numpy`, `tqdm`, `matplotlib`, etc.\n",
    "A complete list of dependencies can be found in the `requirements.txt` file at the root of the repository"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67808,
     "status": "ok",
     "timestamp": 1742263995161,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "qSd9om4cjW0R",
    "outputId": "b58b653d-e85d-4ae1-9afe-4bd8c92a535d"
   },
   "source": [
    "!pip install -r requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-YX6bCeYSvq"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vpFbkX67YSvq"
   },
   "source": [
    "import collections\n",
    "from importlib import reload\n",
    "\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "from typing import Union, List, Tuple, Optional, Callable\n",
    "\n",
    "import test_functions\n",
    "import KingAndCourtesanEnv as kac\n",
    "import joblib\n",
    "from importlib import reload\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_lq5oxThYSvr"
   },
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "chTVmEAgYSvr"
   },
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hnFbNdosYSvr"
   },
   "source": [
    "sns.set_context(\"talk\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fKZ3UpKaYSvr"
   },
   "source": [
    "PLOTS_DIR = Path(\"figs/\") / \"RL Agents\"  # Where to save plots (.png or .svg files)\n",
    "MODELS_DIR = Path(\"models/\") / \"RL Agents\"  # Where to save models (.pth files)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FWYSqaXuYSvr"
   },
   "source": [
    "if not PLOTS_DIR.exists():\n",
    "    PLOTS_DIR.mkdir(parents=True)\n",
    "if not MODELS_DIR.exists():\n",
    "    MODELS_DIR.mkdir(parents=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NpAYwEHBYSvr"
   },
   "source": [
    "DEFAULT_NUMBER_OF_TRAININGS = 3\n",
    "BOARD_SIZE = 6"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtO2lKdmYSvs"
   },
   "source": [
    "## PyTorch setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tWBf6yXAYSvs"
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# Set the device to CUDA if available, otherwise use CPU"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1742264000595,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "e86WohFKYSvs",
    "outputId": "75b2f321-3a38-42a4-d2ef-f824a48c0055"
   },
   "source": [
    "print(\"Available GPUs:\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"- Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Using Metal Performance Shaders (MPS)\")\n",
    "    print(f\"{torch.mps.device_count()} GPU(s) available.\")\n",
    "else:\n",
    "    print(\"- No GPU available.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1742264000602,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "lrB_x1MsYSvs",
    "outputId": "e5c94ba1-aa80-4698-ab8e-dfe681eec180"
   },
   "source": [
    "print(f\"PyTorch will train and test neural networks on {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742264000610,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "IkRyN0_4lHVC",
    "outputId": "26420517-08f3-4972-a969-4fe36f9376b9"
   },
   "source": [
    "# Function to keep the session active\n",
    "def keep_alive():\n",
    "    import IPython\n",
    "    import time\n",
    "    from IPython.display import clear_output\n",
    "    import datetime\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    def refresh_session():\n",
    "        clear_output(wait=True)\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - start_time\n",
    "        hours, rem = divmod(elapsed, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "        print(f\"Session active depuis {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "        print(f\"Traitement en cours... Dernière activité: {datetime.datetime.now().strftime('%H:%M:%S')}\")\n",
    "        return IPython.display.HTML(\n",
    "            \"<script>setTimeout(()=>{google.colab.kernel.invokeFunction('notebook.keep_alive')}, 60000)</script>\")\n",
    "\n",
    "    IPython.display.display(refresh_session())\n",
    "    IPython.get_ipython().kernel.comm_manager.register_target('notebook',\n",
    "                                                              lambda comm, msg: comm.on_msg(\n",
    "                                                                  lambda msg: refresh_session()))\n",
    "\n",
    "\n",
    "keep_alive()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylbNrngZYSvs"
   },
   "source": [
    "## 1. Python King And Courtesan Environment Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mx1WV1y1YSvt"
   },
   "source": [
    "#### The environement and ID Alpha Beta client are implemented in modules `KingAndCourtesanEnv.py` and `IDAlphaBetaClient.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QMZjPUKYSvt"
   },
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1741368240721,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "K55ddJu1YSvt",
    "outputId": "c0f63d32-e492-4e93-bca7-63de23814851"
   },
   "source": [
    "env = kac.KingAndCourtesanEnv(host='localhost', port=3, render_mode='human')\n",
    "\n",
    "# Use environment for training/testing\n",
    "state, info = env.reset()\n",
    "print(f\"Available legal moves: {info['legal_moves']}\")\n",
    "\n",
    "# For rendering\n",
    "env.render()\n",
    "\n",
    "# Close when done\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1741368246352,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "UR7Qhv2nYSvt",
    "outputId": "8ad7b1d9-818a-49d7-d4ac-dcaf58c1162a"
   },
   "source": [
    "info"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn9e1A6gYSvt"
   },
   "source": [
    "#### Testing the environment with two random policies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1741368267465,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "UihId4EGYSvt",
    "outputId": "df5ef28c-88e4-414a-a856-d3f83df44b47"
   },
   "source": [
    "env = kac.KingAndCourtesanEnv(host='localhost', port=3, render_mode='human')\n",
    "\n",
    "_, _ = env.reset()\n",
    "\n",
    "for t in range(50):\n",
    "    print(\"step\", t)\n",
    "    action = env.sample_legal_action()\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Game Over\")\n",
    "        break\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlaL5J74YSvt"
   },
   "source": [
    "### Testing a random agent against ID Alpha-Beta agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4981,
     "status": "ok",
     "timestamp": 1741368302454,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "LpSLyocpYSvt",
    "outputId": "43048515-b112-4618-ddb9-821356d729e8"
   },
   "source": [
    "result = test_functions.test_random_vs_alphabeta(\n",
    "    env_port=3,\n",
    "    agent_port=9,\n",
    "    render_mode='human',\n",
    "    delay_between_moves=1.0,\n",
    "    response_timeout=50,\n",
    ")\n",
    "\n",
    "print(\"\\nGame Results:\")\n",
    "print(f\"Winner: {result['winner']}\")\n",
    "print(f\"Steps: {result['steps']}\")\n",
    "print(f\"Random played as: {result['random_player_role']}\")\n",
    "print(f\"Alpha-Beta played as: {result['alphabeta_player_role']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjsGflMHYSvt"
   },
   "source": [
    "## 2. Deep value-based Reinforcement Learning with Deep Q-Networks (DQN)\n",
    "## Deep Q-Networks v2 (DQN version 2015) with infrequent weight updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Q6-LDE3YSvt"
   },
   "source": [
    "## 2.1. The Q-network"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1742281498766,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "ex7owrPiYSvu"
   },
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Q-Network for King and Courtesan game.\"\"\"\n",
    "\n",
    "    def __init__(self, board_size=6, action_space_size=None):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Input: 6 channels representing the board state\n",
    "        self.board_size = board_size\n",
    "        if action_space_size is None:\n",
    "            self.action_space_size = board_size * board_size * board_size * board_size\n",
    "        else:\n",
    "            self.action_space_size = action_space_size\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(6, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Calculate the size after convolutions\n",
    "        conv_output_size = 128 * board_size * board_size\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(conv_output_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, self.action_space_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with batch normalization and ReLU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # Output layer (no activation, as we want raw Q-values)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_params(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The parameters of the model.\n",
    "        \"\"\"\n",
    "        # return self.params.copy()\n",
    "        return torch.nn.utils.parameters_to_vector(self.parameters()).detach().cpu().numpy()\n",
    "\n",
    "    def set_params(self, params: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Set the parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : np.ndarray\n",
    "            The parameters of the model.\n",
    "        \"\"\"\n",
    "        #self.params = params.copy()\n",
    "        flat_params = torch.tensor(params, dtype=torch.float32).to(device)\n",
    "        torch.nn.utils.vector_to_parameters(flat_params, self.parameters())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNm0xFZ2YSvu"
   },
   "source": [
    "### 2.1.1 Inference Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48bfSY_PYSvu"
   },
   "source": [
    "#### Testing on the untrained agent against itself"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kS1wAmwLYSvu"
   },
   "source": [
    "render_mode = 'human'\n",
    "num_episodes = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Uo5ob_kKYSvu",
    "outputId": "ed6d09a0-a07d-4c86-b278-06b1673e79ab"
   },
   "source": [
    "# Initialize Q-Networks\n",
    "q_network = QNetwork(board_size=BOARD_SIZE).to(device)\n",
    "q_network_adversary = QNetwork(board_size=BOARD_SIZE).to(device)\n",
    "\n",
    "print(f\"Starting Q-Network vs Q-Network test...\")\n",
    "print(f\"Board size: {BOARD_SIZE}x{BOARD_SIZE}\")\n",
    "\n",
    "# Run test\n",
    "stats = test_functions.test_q_network_agent(\n",
    "    env_port=9,\n",
    "    q_network=q_network,\n",
    "    q_network_adversary=q_network_adversary,\n",
    "    num_episode=num_episodes,\n",
    "    render=(render_mode == 'human'),\n",
    ")\n",
    "\n",
    "print(\"\\nGame Results:\")\n",
    "print(f\"Winner: {stats['winner']}\")\n",
    "print(f\"Steps: {stats['steps']}\")\n",
    "print(f\"Main Network played as: {stats['main_network_role']}\")\n",
    "print(f\"Adversary Network played as: {stats['adversary_role']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww_jCh0OYSvu"
   },
   "source": [
    "#### Testing the untrained agent against random agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ejbcL7L4YSvu",
    "outputId": "b6b2a353-7fe9-4f63-bfea-f3fabb352ee0"
   },
   "source": [
    "q_network = QNetwork(board_size=BOARD_SIZE).to(device)\n",
    "results = test_functions.test_q_network_vs_random(env_port=3, num_episodes=5, q_network=q_network)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2h6A0iXkYSvv"
   },
   "source": [
    "### Epsilon Greedy Function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WqMlTFCDYSvv"
   },
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    An Epsilon-Greedy policy.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    epsilon : float\n",
    "        The initial probability of choosing a random action.\n",
    "    epsilon_min : float\n",
    "        The minimum probability of choosing a random action.\n",
    "    epsilon_decay : float\n",
    "        The decay rate for the epsilon value after each action.\n",
    "    env : gym.Env\n",
    "        The environment in which the agent is acting.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-Network used to estimate action values.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(state: np.ndarray) -> np.int64\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "    decay_epsilon()\n",
    "        Decay the epsilon value after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            epsilon_start: float,\n",
    "            epsilon_min: float,\n",
    "            epsilon_decay: float,\n",
    "            env: gym.Env,\n",
    "            q_network: torch.nn.Module,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of EpsilonGreedy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon_start : float\n",
    "            The initial probability of choosing a random action.\n",
    "        epsilon_min : float\n",
    "            The minimum probability of choosing a random action.\n",
    "        epsilon_decay : float\n",
    "            The decay rate for the epsilon value after each episode.\n",
    "        env : gym.Env\n",
    "            The environment in which the agent is acting.\n",
    "        q_network : torch.nn.Module\n",
    "            The Q-Network used to estimate action values.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "\n",
    "    def __call__(self, state: torch.Tensor) -> np.int64:\n",
    "        \"\"\"\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "\n",
    "        If a randomly chosen number is less than epsilon, a random action is chosen.\n",
    "        Otherwise, the action with the highest estimated action value is chosen.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : numpy.ndarray\n",
    "            The current state of the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.int64\n",
    "            The chosen action.\n",
    "        \"\"\"\n",
    "\n",
    "        legal_moves = self.env.legal_moves\n",
    "\n",
    "        # Exploration: random legal move\n",
    "        if random.random() < self.epsilon:\n",
    "            # Randomly select from legal moves\n",
    "            return self.env._move_to_index(random.choice(legal_moves))\n",
    "\n",
    "        # Exploitation: best legal move based on Q-values\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "\n",
    "                # Create legal moves mask\n",
    "                mask = torch.ones_like(q_values) * float('-inf')\n",
    "                for move in legal_moves:\n",
    "                    move_idx = self.env._move_to_index(move)\n",
    "                    mask[0, move_idx] = 0\n",
    "\n",
    "                # Apply mask to get legal Q-values\n",
    "                masked_q_values = q_values + mask\n",
    "\n",
    "                # Find the maximum Q-value\n",
    "                max_q_value = masked_q_values.max().item()\n",
    "\n",
    "                # Find all indices that have the maximum Q-value\n",
    "                max_indices = torch.where(masked_q_values[0] == max_q_value)[0]\n",
    "\n",
    "                # Randomly select one of the max indices\n",
    "                selected_index = max_indices[torch.randint(0, len(max_indices), (1,))].item()\n",
    "\n",
    "        return selected_index\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay the epsilon value after each episode.\n",
    "\n",
    "        The new epsilon value is the maximum of `epsilon_min` and the product of the current\n",
    "        epsilon value and `epsilon_decay`.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpZVAe84YSvv"
   },
   "source": [
    "### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GAkTLMs-YSvv"
   },
   "source": [
    "class MinimumExponentialLR(torch.optim.lr_scheduler.ExponentialLR):\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer: torch.optim.Optimizer,\n",
    "            lr_decay: float,\n",
    "            last_epoch: int = -1,\n",
    "            min_lr: float = 1e-6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of MinimumExponentialLR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            The optimizer whose learning rate should be scheduled.\n",
    "        lr_decay : float\n",
    "            The multiplicative factor of learning rate decay.\n",
    "        last_epoch : int, optional\n",
    "            The index of the last epoch. Default is -1.\n",
    "        min_lr : float, optional\n",
    "            The minimum learning rate. Default is 1e-6.\n",
    "        \"\"\"\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, lr_decay, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute learning rate using chainable form of the scheduler.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[float]\n",
    "            The learning rates of each parameter group.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            max(base_lr * self.gamma ** self.last_epoch, self.min_lr)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9L_8RUxYSvw"
   },
   "source": [
    "### 2.1.2. Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytiDZ4WcYSvw"
   },
   "source": [
    "To compute the target value, we can eliminate the need for an if statement to differentiate between terminal and non-terminal states by using the following formula:\n",
    "\n",
    "$$\n",
    "y = r + \\gamma \\max_{\\mathbf{a}^\\star \\in \\mathcal{A}} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'})_{\\mathbf{a}^\\star} \\times (1 - \\text{done})\n",
    "$$\n",
    "\n",
    "where $\\text{done} = 1$ if $s'$ is a terminal state and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftiYWq13YSvw"
   },
   "source": [
    "### Replay Buffer\n",
    "\n",
    "Memory buffer where experiences are stored. We sample a random batch of experiences from this buffer to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tcickLJeYSvw"
   },
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A Replay Buffer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    buffer : collections.deque\n",
    "        A double-ended queue where the transitions are stored.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    add(state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool)\n",
    "        Add a new transition to the buffer.\n",
    "    sample(batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "        Sample a batch of transitions from the buffer.\n",
    "    __len__()\n",
    "        Return the current size of the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int, env: kac.KingAndCourtesanEnv):\n",
    "        \"\"\"\n",
    "        Initializes a ReplayBuffer instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            The maximum number of transitions that can be stored in the buffer.\n",
    "        env : KingAndCourtesanEnv\n",
    "            The environment\n",
    "        \"\"\"\n",
    "        self.buffer: collections.deque = collections.deque(maxlen=capacity)\n",
    "        self.env = env\n",
    "        self.action_space_size = self.env.action_space.n\n",
    "\n",
    "    def add(\n",
    "            self,\n",
    "            state: torch.Tensor,\n",
    "            action: np.int64,\n",
    "            reward: float,\n",
    "            next_state: np.ndarray,\n",
    "            done: bool,\n",
    "            legal_moves\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add a new transition to the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : numpy.ndarray\n",
    "            The state array of the added state.\n",
    "        action : np.int64\n",
    "            The action of the added transition.\n",
    "        reward : float\n",
    "            The reward of the added transition.\n",
    "        next_state : numpy.ndarray\n",
    "            The next state vector of the added transition.\n",
    "        done : bool\n",
    "            The final state of the added transition.\n",
    "        legal_moves : list\n",
    "            The next legal moves\n",
    "        \"\"\"\n",
    "        if legal_moves is not None:\n",
    "            # Store move indices rather than move strings\n",
    "            legal_move_indices = [self.env._move_to_index(move) for move in legal_moves]\n",
    "            # legal moves actually correspond to the next state's legal moves, so adversary turn\n",
    "            legal_move_indices = set(range(self.action_space_size)) - set(legal_move_indices)\n",
    "            self.buffer.append((state, action, reward, next_state, done, list(legal_move_indices)))\n",
    "\n",
    "        else:\n",
    "            self.buffer.append((state, action, reward, next_state, done, []))\n",
    "\n",
    "    def sample(\n",
    "            self, batch_size: int\n",
    "    ) -> Tuple[np.ndarray, Tuple[int], Tuple[float], np.ndarray, Tuple[bool]]:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The number of transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, float, float, np.ndarray, bool, List[int]]\n",
    "            A batch of `batch_size` transitions.\n",
    "        \"\"\"\n",
    "        # Here, `random.sample(self.buffer, batch_size)`\n",
    "        # returns a list of tuples `(state, action, reward, next_state, done)`\n",
    "        # where:\n",
    "        # - `state`  and `next_state` are numpy arrays\n",
    "        # - `action` and `reward` are floats\n",
    "        # - `done` is a boolean\n",
    "        #\n",
    "        # `states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))`\n",
    "        # generates 5 tuples `state`, `action`, `reward`, `next_state` and `done`, each having `batch_size` elements.\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones, legal_moves_batch = zip(*samples)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "        # Create legal moves mask tensor (batch_size x action_space_size)\n",
    "        # Initialize with large negative value to mask illegal moves\n",
    "        legal_moves_mask = torch.ones((batch_size, self.action_space_size), device=device) * float('-inf')\n",
    "\n",
    "        # Fill in mask for each sample in batch\n",
    "        for i, moves in enumerate(legal_moves_batch):\n",
    "            if moves:  # Check if we have legal moves stored\n",
    "                legal_moves_mask[i, moves] = 0  # Set 0 for legal moves\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, legal_moves_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eX6S66e3YSvw"
   },
   "source": [
    "In 2015, DeepMind further advanced the field of reinforcement learning with the publication of the paper \"Human-level control through deep reinforcement learning\" by Volodymyr Mnih and colleagues (https://www.nature.com/articles/nature14236). This work introduced the second version of Deep Q-Networks (DQN).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/assets/lab6_dqn_nature_journal.jpg\" width=\"200px\" />\n",
    "\n",
    "The key contribution of this paper was the introduction of a method to stabilize the learning process by infrequently updating the target weights. This technique, known as *infrequent updates of target weights*, significantly improved the stability of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGc5JnncYSvw"
   },
   "source": [
    "#### Infrequent weight updates\n",
    "\n",
    "Infrequent weight updates, also known as the use of a target network, is a technique used in Deep Q-Networks (DQN) to address the issue of learning from a moving target.\n",
    "\n",
    "In a typical DQN setup, there are two neural networks: the Q-network and the target network. The Q-network is used to predict the Q-values and is updated at every time step. The target network is used to compute the target Q-values for the update, and its weights are updated less frequently, typically every few thousand steps, by copying the weights from the Q-network.\n",
    "\n",
    "The idea behind infrequent weight updates is to stabilize the learning process by keeping the target Q-values fixed for a number of steps. This mitigates the issue of learning from a moving target, as the target Q-values remain fixed between updates.\n",
    "\n",
    "Without infrequent weight updates, both the predicted and target Q-values would change at every step, which could lead to oscillations and divergence in the learning process. By introducing a delay between updates of the target Q-values, the risk of such oscillations is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMN70UHXYSvw"
   },
   "source": [
    "#### DQN v2015 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-dN4XVAYSvx"
   },
   "source": [
    "Note: main differences with the previous algorithm are highlighted in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qhh0_TSVYSvx"
   },
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br>\n",
    "\t$\\quad\\quad$ capacity of the experience replay memory $M$<br>\n",
    "\t$\\quad\\quad$ batch size $m$<br>\n",
    "\t$\\quad\\quad$ target network update frequency $\\color{red}{\\tau}$<br><br>\n",
    "\n",
    "<b>Initialize</b> replay memory $\\mathcal{D}$ to capacity $M$<br>\n",
    "<b>Initialize</b> action-value function $\\hat{Q}_{\\mathbf{\\omega_1}}$ with random weights $\\mathbf{\\omega_1}$<br>\n",
    "<b>Initialize</b> target action-value function $\\hat{Q}_{\\mathbf{\\omega_2}}$ with weights $\\color{red}{\\mathbf{\\omega_2} = \\mathbf{\\omega_1}}$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega_1}})$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ Store transition $(\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s'})$ in $\\mathcal{D}$<br>\n",
    "\t\t$\\quad\\quad$ If $\\mathcal{D}$ contains \"enough\" transitions<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Sample random batch of transitions $(\\mathbf{s}_j, \\mathbf{a}_j, r_j, \\mathbf{s'}_j)$ from $\\mathcal{D}$ with $j=1$ to $m$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ For each $j$, set $y_j =\n",
    "\t\t\t\\begin{cases}\n",
    "\t\t\tr_j & \\text{for terminal } \\mathbf{s'}_j\\\\\n",
    "\t\t\tr_j + \\gamma \\max_{\\mathbf{a}^\\star} \\hat{Q}_{\\mathbf{\\omega_{\\color{red}{2}}}} (\\mathbf{s'}_j)_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}_j\n",
    "\t\t\t\\end{cases}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Perform a gradient descent step on $\\left( y_j - \\hat{Q}_{\\mathbf{\\omega_1}}(\\mathbf{s}_j)_{\\mathbf{a}_j} \\right)^2$ with respect to the weights $\\mathbf{\\omega_1}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Every $\\color{red}{\\tau}$ steps reset $\\hat{Q}_{\\mathbf{\\omega_2}}$ to $\\hat{Q}_{\\mathbf{\\omega_1}}$, i.e., set $\\color{red}{\\mathbf{\\omega_2} \\leftarrow \\mathbf{\\omega_1}}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega_1}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmsuPeOxYSvx"
   },
   "source": [
    "Infrequent weight updates in the training function:\n",
    "\n",
    "1. **Update the Target Network Infrequently**: Instead of updating the weights of the target network at every time step, update them less frequently, for example, every few thousand steps. The weights of the target network are updated by copying the weights from the Q-network.\n",
    "\n",
    "2. **Compute Target Q-values with the Target Network**: When computing the target Q-values for the update, use the target network instead of the Q-network. This ensures that the target Q-values remain fixed between updates, which stabilizes the learning process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_dqn2_agent(\n",
    "        env,\n",
    "        q_network,\n",
    "        q_network_adversary,\n",
    "        target_q_network,\n",
    "        target_q_network_adversary,\n",
    "        optimizer,\n",
    "        optimizer_adversary,\n",
    "        loss_fn,\n",
    "        epsilon_greedy,\n",
    "        epsilon_greedy_adversary=None,  # Added parameter for adversary's exploration policy\n",
    "        lr_scheduler=None,\n",
    "        lr_scheduler_adversary=None,\n",
    "        num_episodes=5000,\n",
    "        gamma=0.99,\n",
    "        batch_size=256,\n",
    "        replay_buffer=None,\n",
    "        replay_buffer_adversary=None,\n",
    "        target_q_network_sync_period=1000,\n",
    "        grad_clip=10.0,  # Added gradient clipping\n",
    "        double_q=True,  # Added Double-Q learning option\n",
    "        soft_update=False,  # Added soft update option\n",
    "        tau=0.005,  # Polyak averaging factor for soft updates\n",
    "        init_exploration_steps=5000,  # Initial random exploration steps\n",
    "        reward_scale=1.0,  # Reward scaling factor\n",
    "        eval_frequency=100,  # How often to evaluate and print\n",
    "        checkpoint_dir='checkpoints/dqnv2',\n",
    "        checkpoint_frequency=50,\n",
    "        save_best_only=False,\n",
    "        verbose=True,\n",
    "        heuristic_scale=0.3,  # Scaling for heuristic-based rewards\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced DQN agent training with self-play and additional optimizations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    env : KingAndCourtesanEnv\n",
    "        The game environment\n",
    "    q_network, q_network_adversary : QNetwork\n",
    "        The main and adversary Q-Networks\n",
    "    target_q_network, target_q_network_adversary : QNetwork\n",
    "        Target networks for more stable learning\n",
    "    optimizer, optimizer_adversary : torch.optim.Optimizer\n",
    "        Optimizers for both networks\n",
    "    loss_fn : callable\n",
    "        Loss function for training\n",
    "    epsilon_greedy, epsilon_greedy_adversary : EpsilonGreedy\n",
    "        Exploration policies for both agents\n",
    "    device : torch.device\n",
    "        Device for computation\n",
    "    lr_scheduler, lr_scheduler_adversary : torch.optim.lr_scheduler\n",
    "        Learning rate schedulers\n",
    "    num_episodes : int\n",
    "        Number of training episodes\n",
    "    gamma : float\n",
    "        Discount factor\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    replay_buffer, replay_buffer_adversary : ReplayBuffer\n",
    "        Experience replay buffers for both agents\n",
    "    target_q_network_sync_period : int\n",
    "        How often to update target networks\n",
    "    grad_clip : float\n",
    "        Maximum gradient norm for clipping\n",
    "    double_q : bool\n",
    "        Whether to use Double-Q learning\n",
    "    soft_update : bool\n",
    "        Whether to use soft updates for target networks\n",
    "    tau : float\n",
    "        Soft update interpolation factor\n",
    "    init_exploration_steps : int\n",
    "        Number of initial steps with purely random actions\n",
    "    reward_scale : float\n",
    "        Factor to scale rewards by\n",
    "    eval_frequency : int\n",
    "        How often to evaluate and print\n",
    "    checkpoint_dir : str\n",
    "        Directory for checkpointing\n",
    "    checkpoint_frequency : int\n",
    "        How often to save checkpoints\n",
    "    save_best_only : bool\n",
    "        Whether to save only the best model\n",
    "    verbose : bool\n",
    "        Wether to print progress information\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[List[float], List[float], List[float]]\n",
    "        Episode win rates and losses\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_win_rate = float('-inf')\n",
    "    # Use the same epsilon policy for adversary if not provided\n",
    "    if epsilon_greedy_adversary is None:\n",
    "        epsilon_greedy_adversary = epsilon_greedy\n",
    "\n",
    "    iteration = 0\n",
    "    episodes_win_rates = []\n",
    "    main_network_episodes_losses = []\n",
    "    adversary_network_episodes_losses = []\n",
    "\n",
    "    # Initialize exploration counters\n",
    "    exploration_steps = 0\n",
    "    pure_exploration = exploration_steps < init_exploration_steps\n",
    "\n",
    "    # Create running stats for loss tracking\n",
    "    running_loss_main = 0.0\n",
    "    running_loss_adv = 0.0\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes + 1)):\n",
    "        # Switch roles every episode to ensure balanced training\n",
    "        main_player_first = (episode_index % 2 == 1)\n",
    "        optimizer_steps = 0\n",
    "        optimizer_adversary_steps = 0\n",
    "\n",
    "        # Reset environment\n",
    "        state, info = env.reset(options={'is_first_player': main_player_first})\n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        # Initial player\n",
    "        current_player = 0  # 0 for first player (RED), 1 for second player (BLUE)\n",
    "\n",
    "        # Episode metrics\n",
    "        episode_steps = 0\n",
    "\n",
    "        while not done:\n",
    "            episode_steps += 1\n",
    "\n",
    "            # Determine which network to use based on current player and first player assignment\n",
    "            is_main_network_turn = (current_player == 0 and main_player_first) or (\n",
    "                    current_player == 1 and not main_player_first)\n",
    "\n",
    "            # Select action based on current player\n",
    "            if pure_exploration:\n",
    "                # During initial exploration, use completely random actions\n",
    "                action = env.sample_legal_action()\n",
    "                exploration_steps += 1\n",
    "                pure_exploration = exploration_steps < init_exploration_steps\n",
    "            else:\n",
    "                # Use epsilon-greedy policies after initial exploration\n",
    "                if is_main_network_turn:\n",
    "                    # Main network's turn\n",
    "                    current_network = q_network\n",
    "                    current_epsilon = epsilon_greedy\n",
    "                else:\n",
    "                    # Adversary network's turn\n",
    "                    current_network = q_network_adversary\n",
    "                    current_epsilon = epsilon_greedy_adversary\n",
    "\n",
    "                # Use appropriate epsilon-greedy policy\n",
    "                current_epsilon.q_network = current_network\n",
    "                action = current_epsilon(state)\n",
    "\n",
    "            # Take action in environment\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Scaling the reward differently based on terminal vs non-terminal\n",
    "            if done:\n",
    "                adjusted_reward = reward * reward_scale\n",
    "            else:\n",
    "                # Heuristic-based rewards - applying separate scaling\n",
    "                adjusted_reward = reward * heuristic_scale\n",
    "\n",
    "            # print(\"Adjusted reward: %.3f\" % adjusted_reward)\n",
    "\n",
    "            # Store experience for the current player\n",
    "            current_buffer = replay_buffer if is_main_network_turn else replay_buffer_adversary\n",
    "\n",
    "            current_buffer.add(state, action, adjusted_reward, next_state, done, info['legal_moves'])\n",
    "\n",
    "            # Track reward (from main player's perspective only)\n",
    "            if is_main_network_turn:\n",
    "                episode_reward += reward  # Use original reward for tracking\n",
    "\n",
    "            # Train networks if enough experiences and past initial exploration\n",
    "            if not pure_exploration:\n",
    "                # Train main network if enough experiences\n",
    "                if len(replay_buffer) > batch_size:\n",
    "                    optimizer_steps += 1\n",
    "                    loss_main = update_network(\n",
    "                        q_network, target_q_network, optimizer,\n",
    "                        replay_buffer, batch_size, gamma, loss_fn,\n",
    "                        grad_clip, double_q\n",
    "                    )\n",
    "                    running_loss_main = loss_main / optimizer_steps\n",
    "\n",
    "                    # Update learning rate if scheduler provided\n",
    "                    if lr_scheduler is not None:\n",
    "                        lr_scheduler.step()\n",
    "\n",
    "                # Train adversary network if enough experiences\n",
    "                if len(replay_buffer_adversary) > batch_size:\n",
    "                    optimizer_adversary_steps += 1\n",
    "                    loss_adv = update_network(\n",
    "                        q_network_adversary, target_q_network_adversary, optimizer_adversary,\n",
    "                        replay_buffer_adversary, batch_size, gamma, loss_fn,\n",
    "                        grad_clip, double_q\n",
    "                    )\n",
    "                    running_loss_adv = loss_adv / optimizer_adversary_steps\n",
    "\n",
    "                    # Update learning rate if scheduler provided\n",
    "                    if lr_scheduler_adversary is not None:\n",
    "                        lr_scheduler_adversary.step()\n",
    "\n",
    "                # Update target networks\n",
    "                iteration += 1\n",
    "                if soft_update:\n",
    "                    # Soft update of target networks\n",
    "                    soft_update_target_network(q_network, target_q_network, tau)\n",
    "                    soft_update_target_network(q_network_adversary, target_q_network_adversary, tau)\n",
    "                elif iteration % target_q_network_sync_period == 0:\n",
    "                    # Hard update of target networks\n",
    "                    target_q_network.load_state_dict(q_network.state_dict())\n",
    "                    target_q_network_adversary.load_state_dict(q_network_adversary.state_dict())\n",
    "\n",
    "            # Prepare for next step\n",
    "            state = next_state\n",
    "            current_player = 1 - current_player  # Switch player\n",
    "\n",
    "        # Decay exploration rates\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "        if epsilon_greedy_adversary is not epsilon_greedy:\n",
    "            epsilon_greedy_adversary.decay_epsilon()\n",
    "\n",
    "        # Print progress periodically\n",
    "        if (episode_index + 1) % eval_frequency == 0 or episode_index == 0:\n",
    "            print(f\"\\nEpisode {episode_index + 1}/{num_episodes}\")\n",
    "            print(f\"Epsilon: {epsilon_greedy.epsilon:.4f}\")\n",
    "            print(f\"Running Loss - Main: {running_loss_main:.4f}, Adv: {running_loss_adv:.4f}\")\n",
    "            if lr_scheduler is not None:\n",
    "                print(f\"Learning rate - Main: {lr_scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "            test_win_rate = (\n",
    "                test_functions.test_q_network_vs_alpha_beta(env_port=3, agent_port=9, num_episodes=1,\n",
    "                                                            q_network=q_network,\n",
    "                                                            ))['q_network_win_rate']\n",
    "\n",
    "            episodes_win_rates.append(test_win_rate)\n",
    "\n",
    "            # Print progress if verbose\n",
    "            if verbose:\n",
    "                print(f\"\\nEpisode {episode_index + 1}/{num_episodes}\")\n",
    "                print(f\"Win rate: {test_win_rate:.4f}\")\n",
    "                print(f\"Learning Rate: {lr_scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "            # Save best model if performance improved\n",
    "            if test_win_rate > best_win_rate:\n",
    "                best_win_rate = test_win_rate\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "                torch.save({\n",
    "                    'episode': episode_index,\n",
    "                    'q_network_state_dict': q_network.state_dict(),\n",
    "                    'q_network_adversary_state_dict': q_network_adversary.state_dict(),\n",
    "                    'target_q_network_state_dict': target_q_network.state_dict(),\n",
    "                    'target_q_network_adversary_state_dict': target_q_network_adversary.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'optimizer_adversary_state_dict': optimizer_adversary.state_dict(),\n",
    "                    'lr_scheduler_state_dict': lr_scheduler.state_dict() if lr_scheduler else None,\n",
    "                    'lr_scheduler_adversary_state_dict': lr_scheduler_adversary.state_dict() if lr_scheduler_adversary else None,\n",
    "                    'epsilon': epsilon_greedy.epsilon,\n",
    "                    'epsilon_adversary': epsilon_greedy_adversary.epsilon if epsilon_greedy_adversary is not epsilon_greedy else None,\n",
    "                    'best_win_rate': best_win_rate\n",
    "                }, checkpoint_path)\n",
    "                print(\n",
    "                    f\"Saved best model with win rate: {best_win_rate:.2f} at episode {episode_index + 1}/{num_episodes}\")\n",
    "\n",
    "        # Save checkpoint at regular intervals\n",
    "        if (\n",
    "                episode_index + 1) % checkpoint_frequency == 0 and not save_best_only and optimizer_steps and optimizer_adversary_steps:\n",
    "            main_network_episodes_losses.append(running_loss_main)\n",
    "            adversary_network_episodes_losses.append(running_loss_adv)\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_episode_{episode_index}.pt')\n",
    "            torch.save({\n",
    "                'episode': episode_index,\n",
    "                'q_network_state_dict': q_network.state_dict(),\n",
    "                'q_network_adversary_state_dict': q_network_adversary.state_dict(),\n",
    "                'target_q_network_state_dict': target_q_network.state_dict(),\n",
    "                'target_q_network_adversary_state_dict': target_q_network_adversary.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'optimizer_adversary_state_dict': optimizer_adversary.state_dict(),\n",
    "                'lr_scheduler_state_dict': lr_scheduler.state_dict() if lr_scheduler else None,\n",
    "                'lr_scheduler_adversary_state_dict': lr_scheduler_adversary.state_dict() if lr_scheduler_adversary else None,\n",
    "                'epsilon': epsilon_greedy.epsilon,\n",
    "                'epsilon_adversary': epsilon_greedy_adversary.epsilon if epsilon_greedy_adversary is not epsilon_greedy else None,\n",
    "                'win_rates_list': episodes_win_rates,\n",
    "                'main_network_episodes_losses': main_network_episodes_losses,\n",
    "                'adversary_network_episodes_losses': adversary_network_episodes_losses\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved checkpoint at episode {episode_index + 1}/{num_episodes}\")\n",
    "\n",
    "    return episodes_win_rates, main_network_episodes_losses, adversary_network_episodes_losses\n",
    "\n",
    "\n",
    "def update_network(\n",
    "        q_network, target_q_network, optimizer, replay_buffer,\n",
    "        batch_size, gamma, loss_fn, grad_clip, double_q\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function to update a network with legal move masking\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    q_network, target_q_network : neural networks\n",
    "    optimizer : optimizer\n",
    "    replay_buffer : experience buffer\n",
    "    env : environment (needed for legal move generation)\n",
    "    batch_size, gamma, loss_fn, grad_clip, double_q, device : other parameters\n",
    "    \"\"\"\n",
    "    # Sample with legal move masks\n",
    "    states, actions, rewards, next_states, dones, legal_moves_mask = replay_buffer.sample(int(batch_size))\n",
    "\n",
    "    # Get current Q-values for selected actions\n",
    "    current_q = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if double_q:\n",
    "            # Double Q-learning with legal move masking\n",
    "            # Use online network for action selection (with masking)\n",
    "            q_values = q_network(next_states)\n",
    "            q_values_masked = q_values + legal_moves_mask\n",
    "            next_actions = q_values_masked.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # Use target network for value estimation\n",
    "            next_q = target_q_network(next_states).gather(1, next_actions).squeeze(1)\n",
    "        else:\n",
    "            # Standard DQN with legal move masking\n",
    "            next_q_values = target_q_network(next_states)\n",
    "            next_q_values_masked = next_q_values + legal_moves_mask\n",
    "            next_q = next_q_values_masked.max(1)[0]\n",
    "\n",
    "        # Compute target Q-values\n",
    "        target_q = rewards + gamma * next_q * (1 - dones)\n",
    "\n",
    "    # Compute loss and update network\n",
    "    loss = loss_fn(current_q, target_q)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Apply gradient clipping\n",
    "    if grad_clip > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(q_network.parameters(), grad_clip)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def soft_update_target_network(source_network, target_network, tau):\n",
    "    \"\"\"Soft update target network: θ_target = τ*θ_source + (1-τ)*θ_target\"\"\"\n",
    "    for target_param, source_param in zip(target_network.parameters(), source_network.parameters()):\n",
    "        target_param.data.copy_(tau * source_param.data + (1.0 - tau) * target_param.data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sd0SXX5AYSvx"
   },
   "source": [
    "### Training\n",
    "\n",
    "We need to instantiate and initialize the two neural networks.\n",
    "\n",
    "A target network that has the same architecture as the Q-network. The weights of the target network are initially copied from the Q-network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TRlZ6LYgYSvx"
   },
   "source": [
    "env = kac.KingAndCourtesanEnv(\n",
    "    port=3,\n",
    ")\n",
    "\n",
    "# Initialize networks\n",
    "dqnv2 = QNetwork(board_size=6).to(device)\n",
    "q_network_adversary = QNetwork(board_size=6).to(device)\n",
    "target_q_network = QNetwork(board_size=6).to(device)\n",
    "target_q_network_adversary = QNetwork(board_size=6).to(device)\n",
    "\n",
    "# Copy initial weights to target networks\n",
    "target_q_network.load_state_dict(dqnv2.state_dict())\n",
    "target_q_network_adversary.load_state_dict(q_network_adversary.state_dict())\n",
    "\n",
    "# Initialize optimizers with weight decay for regularization\n",
    "optimizer = torch.optim.AdamW(\n",
    "    dqnv2.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-5,  # L2 regularization\n",
    "    amsgrad=True\n",
    ")\n",
    "optimizer_adversary = torch.optim.AdamW(\n",
    "    q_network_adversary.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-5,\n",
    "    amsgrad=True\n",
    ")\n",
    "\n",
    "# Initialize cosine annealing learning rate schedulers\n",
    "total_steps = 20000  # Total number of episodes\n",
    "warmup_steps = 1000  # Initial warmup period with higher learning rate\n",
    "\n",
    "# Cosine schedulers with warm restarts every 2000 episodes\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=2000,  # Restart period\n",
    "    T_mult=2,  # Increase period after each restart\n",
    "    eta_min=5e-5  # Minimum learning rate\n",
    ")\n",
    "lr_scheduler_adversary = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer_adversary,\n",
    "    T_0=2000,\n",
    "    T_mult=2,\n",
    "    eta_min=5e-5\n",
    ")\n",
    "\n",
    "# Initialize Huber loss - more robust to outliers than MSE\n",
    "loss_fn = torch.nn.SmoothL1Loss()  # Huber loss\n",
    "\n",
    "# Initialize epsilon-greedy with a more sophisticated decay schedule\n",
    "epsilon_greedy = EpsilonGreedy(\n",
    "    epsilon_start=1.0,  # Start fully exploratory\n",
    "    epsilon_min=0.01,  # Lower minimum for more exploitation in late stages\n",
    "    epsilon_decay=0.9995,  # Slower decay for longer exploration\n",
    "    env=env,\n",
    "    q_network=dqnv2,\n",
    ")\n",
    "\n",
    "# Same epsilon policy for adversary for balanced self-play\n",
    "epsilon_greedy_adversary = EpsilonGreedy(\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_min=0.01,\n",
    "    epsilon_decay=0.9995,\n",
    "    env=env,\n",
    "    q_network=q_network_adversary,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(100000, env)  # Larger buffer to store more diverse experiences\n",
    "replay_buffer_adversary = ReplayBuffer(100000, env)\n",
    "\n",
    "# Train with more sophisticated hyperparameters\n",
    "episodes_win_rates, main_network_episodes_losses, adversary_network_episodes_losses = train_dqn2_agent(\n",
    "    env,\n",
    "    dqnv2,\n",
    "    q_network_adversary,\n",
    "    target_q_network,\n",
    "    target_q_network_adversary,\n",
    "    optimizer,\n",
    "    optimizer_adversary,\n",
    "    loss_fn,\n",
    "    epsilon_greedy,\n",
    "    epsilon_greedy_adversary,\n",
    "    lr_scheduler,\n",
    "    lr_scheduler_adversary,\n",
    "    num_episodes=10000,\n",
    "    gamma=0.99,  # Higher discount factor for longer-term planning\n",
    "    batch_size=512,  # Larger batch size for more stable gradients\n",
    "    replay_buffer=replay_buffer,\n",
    "    replay_buffer_adversary=replay_buffer_adversary,\n",
    "    target_q_network_sync_period=1000,  # Less frequent updates for more stability\n",
    "    grad_clip=10.0,  # Add gradient clipping to prevent explosions\n",
    "    double_q=True,  # Use Double DQN update rule\n",
    "    soft_update=False,  # Instead of hard updates, can switch to soft updates\n",
    "    tau=0.005,  # Polyak averaging factor for soft updates\n",
    "    init_exploration_steps=5000,  # Initial random steps to fill replay buffer\n",
    "    reward_scale=1.0,  # Can scale rewards for better learning dynamics\n",
    "    eval_frequency=1000,  # Evaluate more frequently\n",
    "    heuristic_scale=0.4\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OZlykkIVV5kO"
   },
   "source": [
    "torch.save(dqnv2, MODELS_DIR / \"dqn2_q_network.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "error",
     "timestamp": 1741397804582,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "u0renVYhV5kO",
    "outputId": "709c0a36-9577-449f-e256-16c10572e08d"
   },
   "source": [
    "joblib.dump({'episodes_win_rates': episodes_win_rates, 'main_network_episodes_losses': main_network_episodes_losses,\n",
    "             'adversary_network_episodes_losses': adversary_network_episodes_losses}, 'dqnv2_train_stats')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lcjBrrrGYSvy"
   },
   "source": [
    "dqn2_trains_result_list: List[List[Union[int, float]]] = [[], []]\n",
    "dqn2_trains_result_list[0] = list(range(len(episodes_win_rates)))\n",
    "dqn2_trains_result_list[1] = episodes_win_rates\n",
    "\n",
    "dqn2_trains_result_df = pd.DataFrame(\n",
    "    np.array(dqn2_trains_result_list).T,\n",
    "    columns=[\"num_episodes\", \"episode_reward\"],\n",
    ")\n",
    "dqn2_trains_result_df[\"agent\"] = \"DQN 2015\"\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bACs-cGsYSvy"
   },
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JTVzz1T1YSvy",
    "outputId": "d26eea3b-d5ca-4db9-f22b-43052985a00b"
   },
   "source": [
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    estimator=None,\n",
    "    data=dqn2_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"dqnv2_trains_results.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlHi02VBYSvy"
   },
   "source": [
    "### Testing Against a Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3vGgy0ifYSvy",
    "outputId": "0e87b62f-8199-4a93-d7de-5803309b18f3"
   },
   "source": [
    "results = test_functions.test_q_network_vs_random(env_port=3, num_episodes=5, q_network=dqnv2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Olr0Sh6wYSvy"
   },
   "source": [
    "### Testing Against our Java ID Alpha Beta Agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "9f8b56b48ebf4c669dde520081b7f10e"
     ]
    },
    "id": "2Dwgh06-YSvy",
    "outputId": "ebcc51a8-7100-480d-9162-f165588b77c8"
   },
   "source": [
    "results = test_functions.test_q_network_vs_alpha_beta(env_port=3, num_episodes=1, q_network=dqnv2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBRlq60oYSvy"
   },
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4h2dKk3CYSvy",
    "outputId": "9e929b0f-f62f-41f7-cbc9-6f5912ea228b"
   },
   "source": [
    "train_score_dqn2 = dqn2_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\n",
    "    \"num_episodes\").mean().max()\n",
    "train_score_dqn2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A3wd4D5YSvy"
   },
   "source": [
    "## 3. Deep policy-based Reinforcement Learning with Monte Carlo Policy Gradient (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUQVeZUlYSvy"
   },
   "source": [
    "### The Policy Gradient theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXmM5uqPYSvy"
   },
   "source": [
    "This is a policy gradient method that directly searchs in a family of parameterized policies $\\pi_\\theta$ for the optimal policy.\n",
    "\n",
    "This method performs gradient ascent in the policy space so that the total return is maximized.\n",
    "We will restrict our work to episodic tasks, *i.e.* tasks that have a starting states and last for a finite and fixed number of steps $T$, called horizon.\n",
    "\n",
    "More formally, we define an optimization criterion that we want to maximize:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=1}^T r(s_t,a_t)\\right],$$\n",
    "\n",
    "where $\\mathbb{E}_{\\pi_\\theta}$ means $a \\sim \\pi_\\theta(\\cdot|s)$ and $T$ is the horizon of the episode.\n",
    "In other words, we want to maximize the value of the starting state: $V^{\\pi_\\theta}(s)$.\n",
    "The policy gradient theorem tells us that:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\mathbb{E}_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (a|s) ~ Q^{\\pi_\\theta}(s,a) \\right],\n",
    "$$\n",
    "\n",
    "where the $Q$-function is defined as:\n",
    "\n",
    "$$Q^{\\pi_\\theta}(a|s) = \\mathbb{E}^{\\pi_\\theta} \\left[\\sum_{t=1}^T r(s_t,a_t)|s=s_1, a=a_1\\right].$$\n",
    "\n",
    "The policy gradient theorem is particularly effective because it allows gradient computation without needing to understand the system's dynamics, as long as the $Q$-function for the current policy is computable. By simply applying the policy and observing the one-step transitions, sufficient information is gathered. Implementing a stochastic gradient ascent and substituting $Q^{\\pi_\\theta}(s_t,a_t)$ with a Monte Carlo estimate $R_t = \\sum_{t'=t}^T r(s_{t'},a_{t'})$ for a single trajectory, we derive the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9MpTW2IYSvz"
   },
   "source": [
    "The REINFORCE algorithm, introduced by Williams in 1992, is a Monte Carlo policy gradient method. It updates the policy in the direction that maximizes rewards, using full-episode returns as an unbiased estimate of the gradient. Each step involves generating an episode using the current policy, computing the gradient estimate, and updating the policy parameters. This algorithm is simple yet powerful, and it's particularly effective in environments where the policy gradient is noisy or the dynamics are complex.\n",
    "\n",
    "For further reading and a deeper understanding, refer to Williams' seminal paper (https://link.springer.com/article/10.1007/BF00992696) and the comprehensive text on reinforcement learning by Richard S. Sutton and Andrew G. Barto: \"Reinforcement Learning: An Introduction\", chap.13 (http://incompleteideas.net/book/RLbook2020.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ua6UYXqYSvz"
   },
   "source": [
    "Here is the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xDI94O0YSvz"
   },
   "source": [
    "### Monte Carlo policy gradient (REINFORCE)\n",
    "\n",
    "<b>REQUIRE</b> <br>\n",
    "$\\quad$ A differentiable policy $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    "$\\quad$ A learning rate $\\alpha \\in \\mathbb{R}^+$ <br>\n",
    "<b>INITIALIZATION</b> <br>\n",
    "$\\quad$ Initialize parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ <br>\n",
    "<br>\n",
    "<b>FOR EACH</b> episode <br>\n",
    "$\\quad$ Generate full trace $\\tau = \\{ \\boldsymbol{s}_0, \\boldsymbol{a}_0, r_1, \\boldsymbol{s}_1, \\boldsymbol{a}_1, \\dots, r_T, \\boldsymbol{s}_T \\}$ following $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    "$\\quad$ <b>FOR</b> $~ t=0,\\dots,T-1$ <br>\n",
    "$\\quad\\quad$ $G \\leftarrow \\sum_{k=t}^{T-1} r_k$ <br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha ~ \\underbrace{G ~ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{a}_t|\\boldsymbol{s}_t)}_{\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})}$ <br>\n",
    "<br>\n",
    "<b>RETURN</b> $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XVCq-yFYSvz"
   },
   "source": [
    "### 3.1 Policy Implementation\n",
    "\n",
    "We will implement a stochastic policy to control the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNYAnaSMYSvz"
   },
   "source": [
    "The network takes an input tensor representing the state of the environment and outputs a tensor of action probabilities."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network used as a policy for the REINFORCE algorithm.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        A fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(state: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the PolicyNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, board_size=6, action_space_size=None):\n",
    "        \"\"\"\n",
    "       Initialize a new instance of PolicyNetwork.\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       n_observations : int\n",
    "           The size of the observation space.\n",
    "       n_actions : int\n",
    "           The size of the action space.\n",
    "       \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        # Input: 6 channels representing the board state\n",
    "        self.board_size = board_size\n",
    "        if action_space_size is None:\n",
    "            self.action_space_size = board_size * board_size * board_size * board_size\n",
    "        else:\n",
    "            self.action_space_size = action_space_size\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(6, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Calculate the size after convolutions\n",
    "        conv_output_size = 128 * board_size * board_size\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(conv_output_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, self.action_space_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Calculate the probability of each action for the given state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_tensor : torch.Tensor\n",
    "            The input tensor (state).\n",
    "            The shape of the tensor should be (N, dim),\n",
    "            where N is the number of states vectors in the batch\n",
    "            and dim is the dimension of state vectors.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (the probability of each action for the given state).\n",
    "        \"\"\"\n",
    "        # Convolutional layers with batch normalization and ReLU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # Output layer with logits (will be converted to probabilities with softmax)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_params(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The parameters of the model.\n",
    "        \"\"\"\n",
    "        # return self.params.copy()\n",
    "        return torch.nn.utils.parameters_to_vector(self.parameters()).detach().cpu().numpy()\n",
    "\n",
    "    def set_params(self, params: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Set the parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : np.ndarray\n",
    "            The parameters of the model.\n",
    "        \"\"\"\n",
    "        #self.params = params.copy()\n",
    "        flat_params = torch.tensor(params, dtype=torch.float32).to(device)\n",
    "        torch.nn.utils.vector_to_parameters(flat_params, self.parameters())\n",
    "\n",
    "    def sample_discrete_action(\n",
    "            self,\n",
    "            env,\n",
    "            state: torch.Tensor,\n",
    "            train: bool = False\n",
    "    ) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Sample a discrete action based on the given state and policy network.\n",
    "\n",
    "        This function takes a state and a policy network, and returns a sampled action and its log probability.\n",
    "        The action is sampled from a categorical distribution defined by the output of the policy network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: KingAndCourtesanEnv\n",
    "            The gane environment\n",
    "        state : numpy.ndarray\n",
    "            The state based on which an action needs to be sampled.\n",
    "\n",
    "        train: bool\n",
    "            Wether we're training the network or not\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[int, torch.Tensor]\n",
    "            The sampled action and its log probability.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert state to tensor\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Get action logits from policy network\n",
    "        if not train:\n",
    "            with torch.no_grad():\n",
    "                action_logits = self(state_tensor)\n",
    "        else:\n",
    "            action_logits = self(state_tensor)\n",
    "\n",
    "        # Create mask for legal moves\n",
    "        legal_moves = env.legal_moves\n",
    "        mask = torch.ones_like(action_logits) * float('-inf')\n",
    "\n",
    "        for move in legal_moves:\n",
    "            move_idx = env._move_to_index(move)\n",
    "            mask[0, move_idx] = 0\n",
    "\n",
    "        # Apply mask to logits\n",
    "        masked_logits = action_logits + mask\n",
    "\n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(masked_logits, dim=1)\n",
    "\n",
    "        # Create categorical distribution\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "\n",
    "        # Sample action\n",
    "        action = m.sample()\n",
    "\n",
    "        # Get log probability\n",
    "        log_prob = m.log_prob(action)\n",
    "\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    @classmethod\n",
    "    def sample_one_episode(\n",
    "            cls,\n",
    "            env,\n",
    "            policy_nn,\n",
    "            policy_nn_adversary=None,\n",
    "            main_player_first=True,\n",
    "            max_episode_duration=500,\n",
    "            gamma=0.99,\n",
    "            train=False,\n",
    "            heuristic_scale=0.4,  # scaling heuristic rewards\n",
    "            opponent_reward_weight=0.5  # Weight for opponent's reward\n",
    "    ) -> Tuple[\n",
    "        List[np.ndarray], List[int], List[float], List[torch.Tensor],\n",
    "        List[np.ndarray], List[int], List[float], List[torch.Tensor]\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Execute one episode within the `env` environment utilizing the policy defined by the `policy_nn` parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : gym.Env\n",
    "            The environment to play in.\n",
    "        policy_nn : PolicyNetwork\n",
    "            The policy neural network.\n",
    "        policy_nn_adversary : PolicyNetwork\n",
    "            The adversary policy neural network.\n",
    "        max_episode_duration : int\n",
    "            The maximum duration of the episode.\n",
    "        gamma : float\n",
    "            Discount factor.\n",
    "        train : bool\n",
    "            Wether we're training the networks or not\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[List[np.ndarray], List[int], List[float], List[torch.Tensor]]\n",
    "            The states, actions, rewards, and log probability of action for each time step in the episode.\n",
    "        \"\"\"\n",
    "        policy1_states = []\n",
    "        policy1_actions = []\n",
    "        policy1_rewards = []\n",
    "        policy1_log_probs = []\n",
    "\n",
    "        policy2_states = []\n",
    "        policy2_actions = []\n",
    "        policy2_rewards = []\n",
    "        policy2_log_probs = []\n",
    "\n",
    "        # Tracking move-by-move rewards for both players\n",
    "        all_p1_rewards = []  # Rewards when policy1 makes moves\n",
    "        all_p2_rewards = []  # Rewards when policy2 makes moves\n",
    "        move_indices = []  # Track which player made each move\n",
    "\n",
    "        state, info = env.reset(options={'is_first_player': main_player_first})\n",
    "        done = False\n",
    "        is_policy1_red = env.is_first_player\n",
    "        current_player = 0  # Start with RED (0)\n",
    "\n",
    "        t = 0\n",
    "        while t < max_episode_duration and not done:\n",
    "            is_policy1_turn = (current_player == 0 and is_policy1_red) or \\\n",
    "                              (current_player == 1 and not is_policy1_red)\n",
    "\n",
    "            # Record state and get action\n",
    "            if is_policy1_turn:\n",
    "                policy1_states.append(state)\n",
    "                action, log_prob = policy_nn.sample_discrete_action(env, state, train=train)\n",
    "                policy1_actions.append(action)\n",
    "                policy1_log_probs.append(log_prob)\n",
    "            else:\n",
    "                policy2_states.append(state)\n",
    "                action, log_prob = policy_nn_adversary.sample_discrete_action(env, state, train=train)\n",
    "                policy2_actions.append(action)\n",
    "                policy2_log_probs.append(log_prob)\n",
    "\n",
    "            # Execute action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Store raw reward and player index for this move\n",
    "            if is_policy1_turn:\n",
    "                all_p1_rewards.append(reward)\n",
    "                move_indices.append(1)  # Policy 1 made this move\n",
    "            else:\n",
    "                all_p2_rewards.append(reward if is_policy1_red else -reward)  # Adjust perspective\n",
    "                move_indices.append(2)  # Policy 2 made this move\n",
    "\n",
    "            # Apply heuristic scaling for non-terminal rewards\n",
    "            if not done:\n",
    "                if is_policy1_turn:\n",
    "                    all_p1_rewards[-1] *= heuristic_scale\n",
    "                else:\n",
    "                    all_p2_rewards[-1] *= heuristic_scale\n",
    "\n",
    "            # Prepare for next step\n",
    "            state = next_state\n",
    "            current_player = 1 - current_player\n",
    "            t += 1\n",
    "\n",
    "        # Reconstruct rewards with opponent consideration\n",
    "        p1_idx = 0\n",
    "        p2_idx = 0\n",
    "\n",
    "        for i, player in enumerate(move_indices):\n",
    "            if player == 1:  # Policy 1's move\n",
    "                own_reward = all_p1_rewards[p1_idx]\n",
    "                p1_idx += 1\n",
    "\n",
    "                # Add opponent's negated reward from previous move if available\n",
    "                if i > 0 and move_indices[i - 1] == 2:\n",
    "                    opp_reward = -all_p2_rewards[p2_idx - 1] * opponent_reward_weight\n",
    "                    policy1_rewards.append(own_reward + opp_reward)\n",
    "                else:\n",
    "                    policy1_rewards.append(own_reward)\n",
    "\n",
    "            else:  # Policy 2's move\n",
    "                own_reward = all_p2_rewards[p2_idx]\n",
    "                p2_idx += 1\n",
    "\n",
    "                # Add opponent's negated reward from previous move if available\n",
    "                if i > 0 and move_indices[i - 1] == 1:\n",
    "                    opp_reward = -all_p1_rewards[p1_idx - 1] * opponent_reward_weight\n",
    "                    policy2_rewards.append(own_reward + opp_reward)\n",
    "                else:\n",
    "                    policy2_rewards.append(own_reward)\n",
    "\n",
    "        # Calculate returns\n",
    "        policy1_returns = []\n",
    "        G = 0\n",
    "        for r in reversed(policy1_rewards):\n",
    "            # print(\"Policy 1 reward:\", r)\n",
    "            G = r + gamma * G\n",
    "            policy1_returns.insert(0, G)\n",
    "\n",
    "        policy2_returns = []\n",
    "        G = 0\n",
    "        for r in reversed(policy2_rewards):\n",
    "            # print(\"Policy 2 reward:\", r)\n",
    "            G = r + gamma * G\n",
    "            policy2_returns.insert(0, G)\n",
    "\n",
    "        return (\n",
    "            policy1_states, policy1_actions, policy1_returns, policy1_log_probs,\n",
    "            policy2_states, policy2_actions, policy2_returns, policy2_log_probs\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the untrained agent against itself"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_episodes = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p7yA2QDRYSvz"
   },
   "source": [
    "# Initialize Policy-Networks\n",
    "policy_network = PolicyNetwork(board_size=BOARD_SIZE).to(device)\n",
    "policy_network_adversary = PolicyNetwork(board_size=BOARD_SIZE).to(device)\n",
    "\n",
    "print(f\"Starting Policy-Network vs Policy-Network test...\")\n",
    "print(f\"Board size: {BOARD_SIZE}x{BOARD_SIZE}\")\n",
    "\n",
    "# Run test\n",
    "stats = test_functions.test_policy_network_agent(\n",
    "    env_port=3,\n",
    "    policy_network=policy_network,\n",
    "    policy_network_adversary=policy_network_adversary,\n",
    "    num_episode=num_episodes,\n",
    "    render=(render_mode == 'human'),\n",
    ")\n",
    "\n",
    "print(\"\\nGame Results:\")\n",
    "print(f\"Winner: {stats['winner']}\")\n",
    "print(f\"Steps: {stats['steps']}\")\n",
    "print(f\"Main Network played as: {stats['main_network_role']}\")\n",
    "print(f\"Adversary Network played as: {stats['adversary_role']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOfQXo8fYSvz"
   },
   "source": [
    "#### Testing the untrained agent against random agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KkEBw_WOYSvz",
    "outputId": "705594bd-6ba9-4e1a-9ef1-de5d21985f25"
   },
   "source": [
    "policy_network = PolicyNetwork(board_size=BOARD_SIZE).to(device)\n",
    "results = test_functions.test_policy_network_vs_random(env_port=3, num_episodes=5, policy_network=policy_network)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUZ8umqJYSvz"
   },
   "source": [
    "#### Test Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY_OBv-wYSvz"
   },
   "source": [
    "`avg_return_on_multiple_episodes` function tests the given policy $\\pi_\\theta$ on `num_episodes` episodes (for fixed horizon $T$) and returns the average reward on the `num_episodes` episodes.\n",
    "\n",
    "The function `avg_return_on_multiple_episodes` is designed to play multiple episodes of a given environment using a specified policy neural network and calculate the average return. It takes as input the environment to play in, the policy neural networks to use, the number of episodes to play and the maximum duration of an episode.\n",
    "In each episode, it uses the `sample_one_episode` function to play the episode and collect the rewards. The function then returns the average of these cumulated rewards.\n",
    "\n",
    "`avg_return_on_multiple_episodes` will be used for evaluating the performance of a policy over multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1742283514879,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "7lviUcxCYSvz"
   },
   "source": [
    "def avg_return_on_multiple_episodes(\n",
    "        env: gym.Env,\n",
    "        policy_nn: PolicyNetwork,\n",
    "        policy_nn_adversary: PolicyNetwork,\n",
    "        num_test_episode: int,\n",
    "        max_episode_duration: int | float = 500,\n",
    "        gamma: float = 0.99,\n",
    "        verbose: bool = False,\n",
    "        heuristic_scale: float = 0.3,\n",
    "        opponent_reward_weight: float = 0.5\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Play multiple episodes of the environment and calculate the average return.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to play in.\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy neural network.\n",
    "    policy_nn_adversary : PolicyNetwork\n",
    "        The adversary policy neural network.\n",
    "    num_test_episode : int\n",
    "        The number of episodes to play.\n",
    "    max_episode_duration : int\n",
    "        The maximum duration of an episode.\n",
    "    gamma : float\n",
    "        Discount factor.\n",
    "    verbose : bool\n",
    "        Wether to print detailed information\n",
    "    heuristic_scale : float\n",
    "        Heuristic values scaling factor.\n",
    "    opponent_reward_weight : float\n",
    "        Weight of opponent reward.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The average return.\n",
    "    \"\"\"\n",
    "    policy_network_class = policy_nn.__class__\n",
    "    total_return = 0.0\n",
    "    episode_returns = []\n",
    "\n",
    "    for episode in range(num_test_episode):\n",
    "        if verbose:\n",
    "            print(f\"Evaluating episode {episode + 1}/{num_test_episode}\")\n",
    "\n",
    "        # Sample an episode using the policy networks\n",
    "        returns = policy_network_class.sample_one_episode(\n",
    "            env=env,\n",
    "            policy_nn=policy_nn,\n",
    "            policy_nn_adversary=policy_nn_adversary,\n",
    "            max_episode_duration=max_episode_duration,\n",
    "            gamma=gamma,\n",
    "            heuristic_scale=heuristic_scale,\n",
    "            opponent_reward_weight=opponent_reward_weight\n",
    "        )[2]\n",
    "\n",
    "        # Calculate the episode return\n",
    "        if returns:  # Check if returns list is not empty\n",
    "            # The first element in returns is the cumulative discounted return for the whole episode\n",
    "            episode_return = returns[0]\n",
    "            total_return += episode_return\n",
    "            episode_returns.append(episode_return)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Episode {episode + 1} return: {episode_return:.2f}\")\n",
    "        else:\n",
    "            # Handle case where no returns were collected (e.g., if the main agent never got to act)\n",
    "            if verbose:\n",
    "                print(f\"Episode {episode + 1}: No returns collected\")\n",
    "\n",
    "    # Calculate average return\n",
    "    if episode_returns:\n",
    "        average_return = total_return / len(episode_returns)\n",
    "    else:\n",
    "        average_return = 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Average return over {num_test_episode} episodes: {average_return:.2f}\")\n",
    "        if len(episode_returns) > 1:\n",
    "            print(f\"Min return: {min(episode_returns):.2f}, Max return: {max(episode_returns):.2f}\")\n",
    "\n",
    "    return average_return"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SL9HcKSYSv0"
   },
   "source": [
    "Testing this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NB81sQKkYSv0"
   },
   "source": [
    "port = 3\n",
    "render_mode = 'human'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XH8Hhu1NYSv0"
   },
   "source": [
    "policy_network = PolicyNetwork(board_size=BOARD_SIZE).to(device)\n",
    "policy_network_adversary = PolicyNetwork(board_size=BOARD_SIZE).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vVn84OQZYSv0",
    "outputId": "6030de5c-f376-4a2a-9fc7-1b0ed3fb45c4"
   },
   "source": [
    "env = kac.KingAndCourtesanEnv(port=port, render_mode=render_mode)\n",
    "\n",
    "average_return = avg_return_on_multiple_episodes(env=env, policy_nn=policy_network,\n",
    "                                                 policy_nn_adversary=policy_network_adversary, num_test_episode=5,\n",
    "                                                 verbose=True)\n",
    "\n",
    "print(average_return)\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKKoN6G4YSv0"
   },
   "source": [
    "### 3.2 Train Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtWm0IJnYSv0"
   },
   "source": [
    "`train_reinforce_discrete` trains a policy network using the REINFORCE algorithm in the given environment. This function takes as input the environment, the number of training episodes, the number of tests to perform per episode, the maximum duration of an episode, and the learning rate for the optimizer.\n",
    "\n",
    "The function first initializes a policy network and an AdamW optimizer. Then, for each training episode, it generates an episode using the current policies (current player and adversary) and calculates the return at each time step. It uses this return and the log probability of the action taken at that time step to compute the loss, which is the negative of the product of the return and the log probability. This loss is used to update the policy network parameters using gradient ascent.\n",
    "\n",
    "After each training episode, the function tests the current policy by playing a number of test episodes and calculating the average return. This average return is added to a list for monitoring purposes.\n",
    "\n",
    "The function returns the trained policy network and the list of average returns for each episode. This function encapsulates the main loop of the REINFORCE algorithm, including the policy update step."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_reinforce_discrete(\n",
    "        env: gym.Env,\n",
    "        num_episodes: int = 5000,\n",
    "        max_episode_duration: int | float = 500,\n",
    "        learning_rate: float = 0.001,\n",
    "        gamma: float = 0.99,\n",
    "        entropy_coef: float = 0.01,  # entropy regularization coefficient\n",
    "        grad_clip: float = 10.0,  # gradient clipping threshold\n",
    "        eval_frequency: int = 50,  # evaluation frequency\n",
    "        verbose: bool = False,\n",
    "        checkpoint_dir: str = 'checkpoints/reinforce',\n",
    "        checkpoint_frequency: int = 50,\n",
    "        save_best_only: bool = False,\n",
    "        heuristic_scale=0.3,\n",
    "        opponent_reward_weight: float = 0.5\n",
    ") -> Tuple[PolicyNetwork, List[float], List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Enhanced training function for REINFORCE algorithm with self-play.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train in.\n",
    "    num_episodes : int\n",
    "        The number of training episodes.\n",
    "    num_test_per_episode : int\n",
    "        The number of tests to perform per episode.\n",
    "    max_episode_duration : int\n",
    "        The maximum length of an episode.\n",
    "    learning_rate : float\n",
    "        The initial step size.\n",
    "    gamma : float\n",
    "        Discount factor for future rewards.\n",
    "    entropy_coef : float\n",
    "        Coefficient for entropy regularization.\n",
    "    grad_clip : float\n",
    "        Gradient clipping parameter.\n",
    "    eval_frequency : int\n",
    "        Frequency of evaluation.\n",
    "    verbose : bool\n",
    "        Whether to print detailed progress information.\n",
    "    checkpoint_dir : str\n",
    "        Directory to save checkpoints.\n",
    "    checkpoint_frequency : int\n",
    "        How often to save checkpoints.\n",
    "    save_best_only : bool\n",
    "        Whether to save only the best performing policy\n",
    "    heuristic_scale : float\n",
    "        Heuristic values scaling factor.\n",
    "    opponent_reward_weight : float\n",
    "        Weight of opponent reward.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[PolicyNetwork, List[float], List[float], List[float]]\n",
    "        The final trained policy, episode win rates and losses\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_win_rate = float('-inf')\n",
    "\n",
    "    episodes_win_rates = []\n",
    "    main_network_episodes_losses = []\n",
    "    adversary_network_episodes_losses = []\n",
    "\n",
    "    # Initialize policy networks\n",
    "    policy_nn = PolicyNetwork(board_size=env.board_size).to(device)\n",
    "    policy_nn_adversary = PolicyNetwork(board_size=env.board_size).to(device)\n",
    "\n",
    "    # Initialize optimizers with weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        policy_nn.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=1e-5  # L2 regularization\n",
    "    )\n",
    "    optimizer_adversary = torch.optim.AdamW(\n",
    "        policy_nn_adversary.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "\n",
    "    # Initialize cosine annealing learning rate schedulers with warm restarts\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=500,  # Restart period\n",
    "        T_mult=2,  # Increase period after each restart\n",
    "        eta_min=1e-5  # Minimum learning rate\n",
    "    )\n",
    "    lr_scheduler_adversary = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer_adversary,\n",
    "        T_0=500,\n",
    "        T_mult=2,\n",
    "        eta_min=1e-5\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for episode_index in tqdm(range(num_episodes)):\n",
    "\n",
    "        main_player_first = (episode_index % 2 == 0)\n",
    "        (main_states, main_actions, main_returns, main_log_probs,\n",
    "         adv_states, adv_actions, adv_returns, adv_log_probs) = PolicyNetwork.sample_one_episode(\n",
    "            env=env,\n",
    "            policy_nn=policy_nn,\n",
    "            policy_nn_adversary=policy_nn_adversary,\n",
    "            max_episode_duration=max_episode_duration,\n",
    "            gamma=gamma,\n",
    "            train=True,\n",
    "            heuristic_scale=heuristic_scale,\n",
    "            opponent_reward_weight=opponent_reward_weight\n",
    "        )\n",
    "\n",
    "        # Update main policy if it collected any experiences\n",
    "        if main_actions:\n",
    "            # Normalize returns for more stable training\n",
    "            # if len(main_returns) > 1:\n",
    "            #    main_returns_tensor = torch.FloatTensor(main_returns).to(device)\n",
    "            #   main_returns_normalized = (main_returns_tensor - main_returns_tensor.mean()) / (main_returns_tensor.std() + 1e-8)\n",
    "            # else:\n",
    "            # main_returns_normalized = torch.FloatTensor(main_returns).to(device)\n",
    "\n",
    "            # Compute policy loss with entropy regularization\n",
    "            policy_loss = 0\n",
    "            entropy = 0\n",
    "\n",
    "            # Add baseline normalization for returns\n",
    "            #if main_returns:\n",
    "            main_returns_tensor = torch.tensor(main_returns, device=device)\n",
    "            main_returns_adv_tensor = torch.tensor(adv_returns, device=device)\n",
    "            # if len(main_returns) > 1:\n",
    "            # Normalize returns for stability\n",
    "            main_returns_mean = main_returns_tensor.mean()\n",
    "            main_returns_std = main_returns_tensor.std() + 1e-8\n",
    "            main_returns_normalized = (main_returns_tensor - main_returns_mean) / main_returns_std\n",
    "\n",
    "            main_returns_adv_mean = main_returns_adv_tensor.mean()\n",
    "            main_returns_adv_std = main_returns_adv_tensor.std() + 1e-8\n",
    "            adv_retuns_normalized = (main_returns_adv_tensor - main_returns_adv_mean) / main_returns_adv_std\n",
    "            for t in range(len(main_returns_normalized)):\n",
    "                # Policy gradient loss\n",
    "                policy_loss -= main_log_probs[t] * main_returns_normalized[t]\n",
    "\n",
    "                # Entropy regularization (encourage exploration)\n",
    "                entropy -= main_log_probs[t].exp() * main_log_probs[t]\n",
    "\n",
    "            # Combine losses with coefficients\n",
    "            total_loss = policy_loss - entropy_coef * entropy\n",
    "\n",
    "            # Gradient update for main policy\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Apply gradient clipping\n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(policy_nn.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            if verbose and episode_index % 10 == 0:\n",
    "                print(\n",
    "                    f\"Episode {episode_index + 1} - Main Policy Loss: {policy_loss.item():.4f}, Entropy: {entropy.item():.4f}\")\n",
    "\n",
    "        # ADVERSARY TRAINING\n",
    "        # Sample episode from adversary's perspective\n",
    "\n",
    "        # Update adversary policy if it collected any experiences\n",
    "        if adv_actions:\n",
    "            # Normalize returns for more stable training\n",
    "            # if len(adv_returns) > 1:\n",
    "            #     adv_returns_tensor = torch.FloatTensor(adv_returns).to(device)\n",
    "            #     adv_returns_normalized = (adv_returns_tensor - adv_returns_tensor.mean()) / (adv_returns_tensor.std() + 1e-8)\n",
    "            # else:\n",
    "            #     adv_returns_normalized = torch.FloatTensor(adv_returns).to(device)\n",
    "\n",
    "            # Compute policy loss with entropy regularization\n",
    "            adv_policy_loss = 0\n",
    "            adv_entropy = 0\n",
    "\n",
    "            for t in range(len(adv_retuns_normalized)):\n",
    "                # Policy gradient loss\n",
    "                adv_policy_loss -= adv_log_probs[t] * adv_retuns_normalized[t]\n",
    "\n",
    "                # Entropy regularization (encourage exploration)\n",
    "                adv_entropy -= adv_log_probs[t].exp() * adv_log_probs[t]\n",
    "\n",
    "            # Combine losses with coefficients\n",
    "            adv_total_loss = adv_policy_loss - entropy_coef * adv_entropy\n",
    "\n",
    "            # Gradient update for adversary policy\n",
    "            optimizer_adversary.zero_grad()\n",
    "            adv_total_loss.backward()\n",
    "\n",
    "            # Apply gradient clipping\n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(policy_nn_adversary.parameters(), grad_clip)\n",
    "\n",
    "            optimizer_adversary.step()\n",
    "            lr_scheduler_adversary.step()\n",
    "\n",
    "        # Evaluate periodically\n",
    "        if (episode_index + 1) % eval_frequency == 0:  # or episode_index == 0\n",
    "            print(f\"\\nEpisode {episode_index + 1}/{num_episodes}\")\n",
    "            print(f\"Loss - Main: {total_loss.item():.4f}, Adv: {adv_total_loss.item():.4f}\")\n",
    "            if lr_scheduler is not None:\n",
    "                print(f\"Learning rate - Main: {lr_scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "            test_win_rate = (\n",
    "                test_functions.test_policy_network_vs_alpha_beta(env_port=3, agent_port=9, num_episodes=1,\n",
    "                                                                 policy_network=policy_nn,\n",
    "                                                                 ))['policy_network_win_rate']\n",
    "\n",
    "            episodes_win_rates.append(test_win_rate)\n",
    "\n",
    "            # Print progress if verbose\n",
    "            if verbose:\n",
    "                print(f\"\\nEpisode {episode_index + 1}/{num_episodes}\")\n",
    "                print(f\"Win rate: {test_win_rate:.4f}\")\n",
    "                print(f\"Learning Rate: {lr_scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "            # Save best model if performance improved\n",
    "            if test_win_rate > best_win_rate:\n",
    "                best_win_rate = test_win_rate\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "                torch.save({\n",
    "                    'episode': episode_index,\n",
    "                    'policy_nn_state_dict': policy_nn.state_dict(),\n",
    "                    'policy_nn_adversary_state_dict': policy_nn_adversary.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'optimizer_adversary_state_dict': optimizer_adversary.state_dict(),\n",
    "                    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                    'lr_scheduler_adversary_state_dict': lr_scheduler_adversary.state_dict(),\n",
    "                    'best_win_rate': best_win_rate\n",
    "                }, checkpoint_path)\n",
    "                print(\n",
    "                    f\"Saved best model with win rate: {best_win_rate:.2f} at episode {episode_index + 1}/{num_episodes}\")\n",
    "\n",
    "        # Save checkpoint at regular intervals\n",
    "        if (episode_index + 1) % checkpoint_frequency == 0 and not save_best_only:\n",
    "            main_network_episodes_losses.append(total_loss.item())\n",
    "            adversary_network_episodes_losses.append(adv_total_loss.item())\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_episode_{episode_index + 1}.pt')\n",
    "            torch.save({\n",
    "                'episode': episode_index,\n",
    "                'policy_nn_state_dict': policy_nn.state_dict(),\n",
    "                'policy_nn_adversary_state_dict': policy_nn_adversary.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'optimizer_adversary_state_dict': optimizer_adversary.state_dict(),\n",
    "                'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                'lr_scheduler_adversary_state_dict': lr_scheduler_adversary.state_dict(),\n",
    "                'win_rates_list': episodes_win_rates,\n",
    "                'main_network_episodes_losses': main_network_episodes_losses,\n",
    "                'adversary_network_episodes_losses': adversary_network_episodes_losses\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved checkpoint at episode {episode_index + 1}/{num_episodes}\")\n",
    "\n",
    "    return policy_nn, episodes_win_rates, main_network_episodes_losses, adversary_network_episodes_losses"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYLdZKgaYSv0"
   },
   "source": [
    "#### Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2950144,
     "status": "ok",
     "timestamp": 1742267762060,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "JY_iK3E8YSv0",
    "outputId": "23289510-4d7e-45eb-80c6-77cdba112702"
   },
   "source": [
    "env = kac.KingAndCourtesanEnv(\n",
    "    port=3,\n",
    ")\n",
    "\n",
    "#for train_index in range(NUMBER_OF_TRAININGS):\n",
    "reinforce_policy_nn, episodes_win_rates, main_network_episodes_losses, adversary_network_episodes_losses = train_reinforce_discrete(\n",
    "    env=env,\n",
    "    num_episodes=10000,\n",
    "    max_episode_duration=float('inf'),\n",
    "    learning_rate=0.0005,\n",
    "    gamma=0.99,\n",
    "    entropy_coef=0.05,  # Entropy regularization\n",
    "    grad_clip=1.0,  # Gradient clipping\n",
    "    eval_frequency=5000,  # Evaluate every 50 episodes\n",
    "    verbose=True,\n",
    "    heuristic_scale=0.2,\n",
    "    opponent_reward_weight=0.3,\n",
    "    checkpoint_frequency=1000,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EMd8icPmV5kV"
   },
   "source": [
    "torch.save(reinforce_policy_nn, MODELS_DIR / \"reinforce_policy_network.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1742267905643,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "9st68oTbV5kV",
    "outputId": "e41d836b-ee15-4ae2-f4c0-6c859c27d591"
   },
   "source": [
    "joblib.dump({'episodes_win_rates': episodes_win_rates, 'main_network_episodes_losses': main_network_episodes_losses,\n",
    "             'adversary_network_episodes_losses': adversary_network_episodes_losses}, 'reinforce_train_stats')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1742267928541,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "QHpnR9D0YSv0",
    "outputId": "512076ec-ff80-4840-a8b5-668c494bf351"
   },
   "source": [
    "reinforce_trains_result_list: List[List[Union[int, float]]] = [[], []]\n",
    "reinforce_trains_result_list[0] = list(range(len(episodes_win_rates)))\n",
    "reinforce_trains_result_list[1] = episodes_win_rates\n",
    "\n",
    "reinforce_trains_result_df = pd.DataFrame(\n",
    "    np.array(reinforce_trains_result_list).T,\n",
    "    columns=[\"num_episodes\", \"win_rate\"],\n",
    ")\n",
    "reinforce_trains_result_df[\"agent\"] = \"REINFORCE\"\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcGkOZ-xYSv0"
   },
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1742267940594,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "NRq_U97oYSv0",
    "outputId": "374d63d9-9f5b-4544-d732-7fb16f12a565"
   },
   "source": [
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"win_rate\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    estimator=None,\n",
    "    data=reinforce_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"reinforce_trains_results.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1742268055748,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "NisEQa49YSv0",
    "outputId": "e571a339-07ce-4eaf-ad31-96099bd0f5f7"
   },
   "source": [
    "all_trains_result_df = pd.concat(\n",
    "    [\n",
    "        reinforce_trains_result_df,\n",
    "        reinforce_trains_result_df,\n",
    "    ]\n",
    ")\n",
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"win_rate\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    data=all_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"trains_results_agg.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMV3HH_3YSv0"
   },
   "source": [
    "### Testing the agent against a random policy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210662,
     "status": "ok",
     "timestamp": 1742268506188,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "ibMaJf_lYSv0",
    "outputId": "85c866f1-abbb-4e4b-a6b0-cc1b77e0f3fa"
   },
   "source": [
    "results = test_functions.test_policy_network_vs_random(env_port=3, num_episodes=5, policy_network=reinforce_policy_nn)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0uSrREaYSv1"
   },
   "source": [
    "### Testing the agent against our Java ID Alpha Beta agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pEva11BxYSv1"
   },
   "source": [
    "reinforce_policy_nn = torch.load(\n",
    "    MODELS_DIR / \"reinforce_policy_network.pth\",\n",
    "    map_location=device,\n",
    "    weights_only=False  # Only do this if you trust the source of this file\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = test_functions.test_policy_network_vs_alpha_beta(env_port=3, num_episodes=1,\n",
    "                                                           policy_network=reinforce_policy_nn, agent_port=3,\n",
    "                                                           max_steps=300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#results = test_functions.test_policy_network_vs_alpha_beta(env_port=3, num_episodes=1,\n",
    "#                                                           policy_network=reinforce_policy_nn, agent_port=3, max_steps=300, first_player_policy_network=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqtx_9wYYSv1"
   },
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J3foCmBwYSv1"
   },
   "source": [
    "train_score_reinforce = reinforce_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\n",
    "    \"num_episodes\").mean().max()\n",
    "train_score_reinforce"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZ7ZNbatYSv1"
   },
   "source": [
    "## 4. Gradient-Free Optimization - CEM / ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUG_9t8vYSv1"
   },
   "source": [
    "### 4.1 Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rO0HlLMYSv1"
   },
   "source": [
    "In Reinforcement Learning, by convention the score is a reward to maximize whereas in mathematical optimization the score is a cost to minimize; the objective function will therefore return the opposite of the reward as the score of evaluated policies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ObjectiveFunction:\n",
    "    \"\"\"\n",
    "    Objective function for evaluating a policy in a given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to evaluate the policy.\n",
    "    policy : torch.nn.Module\n",
    "        The policy to evaluate.\n",
    "    adversary_policy : torch.nn.Module\n",
    "        The adversary policy.\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run for each evaluation, by default 1.\n",
    "    max_time_steps : float, optional\n",
    "        The maximum number of time steps per episode, by default float(\"inf\").\n",
    "    minimization_solver : bool, optional\n",
    "        Whether the solver is a minimization solver, by default True.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to evaluate the policy.\n",
    "    policy : torch.nn.Module\n",
    "        The policy to evaluate.\n",
    "    num_episodes : int\n",
    "        The number of episodes to run for each evaluation.\n",
    "    max_time_steps : float\n",
    "        The maximum number of time steps per episode.\n",
    "    minimization_solver : bool\n",
    "        Whether the solver is a minimization solver.\n",
    "    num_evals : int\n",
    "        The number of evaluations performed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            env: gym.Env,\n",
    "            policy: torch.nn.Module,\n",
    "            adversary_policy: torch.nn.Module,\n",
    "            num_episodes: int = 1,\n",
    "            max_time_steps: float = float(\"inf\"),\n",
    "            minimization_solver: bool = True,\n",
    "            heuristic_scale: float = 0.2\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.adversary_policy = adversary_policy\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_time_steps = max_time_steps\n",
    "        self.minimization_solver = minimization_solver\n",
    "        self.heuristic_scale = heuristic_scale\n",
    "\n",
    "        self.num_evals = 0\n",
    "\n",
    "    def eval(self, env, policy_params: np.ndarray, num_episodes: Optional[int] = None,\n",
    "             max_time_steps: Optional[float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate a policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        policy_params : np.ndarray\n",
    "            The parameters of the policy to evaluate.\n",
    "        num_episodes : int, optional\n",
    "            The number of episodes to run for each evaluation, by default None.\n",
    "        max_time_steps : float, optional\n",
    "            The maximum number of time steps per episode, by default None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The average total rewards over the evaluation episodes.\n",
    "        \"\"\"\n",
    "        self.policy.set_params(policy_params)\n",
    "\n",
    "        self.num_evals += 1\n",
    "\n",
    "        if num_episodes is None:\n",
    "            num_episodes = self.num_episodes\n",
    "\n",
    "        if max_time_steps is None:\n",
    "            max_time_steps = self.max_time_steps\n",
    "\n",
    "        average_return = avg_return_on_multiple_episodes(env=env, policy_nn=self.policy,\n",
    "                                                         policy_nn_adversary=self.policy, num_test_episode=num_episodes,\n",
    "                                                         verbose=False, max_episode_duration=max_time_steps,\n",
    "                                                         heuristic_scale=self.heuristic_scale)\n",
    "\n",
    "        # print(average_return)\n",
    "\n",
    "        if self.minimization_solver:\n",
    "            average_return *= -1.0\n",
    "\n",
    "        return average_return  # Optimizers do minimization by default...\n",
    "\n",
    "    def __call__(self, env, policy_params: np.ndarray, num_episodes: Optional[int] = None,\n",
    "                 max_time_steps: Optional[float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate a policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: KingAndCourtesanEnv\n",
    "            The environment in which to evaluate the policy.\n",
    "        policy_params : np.ndarray\n",
    "            The parameters of the policy to evaluate.\n",
    "        num_episodes : int, optional\n",
    "            The number of episodes to run for each evaluation, by default None.\n",
    "        max_time_steps : float, optional\n",
    "            The maximum number of time steps per episode, by default None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The average total rewards over the evaluation episodes.\n",
    "        \"\"\"\n",
    "        return self.eval(env, policy_params, num_episodes=self.num_episodes, max_time_steps=self.max_time_steps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQMH1gw8YSv1"
   },
   "source": [
    "## 4.2 CEM optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4IUsgXEYSv1"
   },
   "source": [
    "`cem_uncorrelated` function searches the best $\\theta$ parameters with a Cross Entropy Method, using the objective function defined above.\n",
    "$\\mathbb{P}$ can be defined as an multivariate normal distribution $\\mathcal{N}\\left( \\boldsymbol{\\mu}, \\boldsymbol{\\sigma^2} \\boldsymbol{\\Sigma} \\right)$ where $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma^2} \\boldsymbol{\\Sigma}$ are vectors i.e. we use one mean and one variance parameters per dimension of $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sRKJYodYSv1"
   },
   "source": [
    "**Cross Entropy**\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad\\quad$ $f$: the objective function<br>\n",
    "$\\quad\\quad$ $\\mathbb{P}$: family of distribution<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta}$: initial parameters for the proposal distribution $\\mathbb{P}$<br>\n",
    "\n",
    "**Algorithm parameter**:<br>\n",
    "$\\quad\\quad$ $m$: sample size<br>\n",
    "$\\quad\\quad$ $m_{\\text{elite}}$: number of samples to use to fit $\\boldsymbol{\\theta}$<br>\n",
    "\n",
    "**FOR EACH** iteration<br>\n",
    "$\\quad\\quad$ samples $\\leftarrow \\{ \\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m \\}$ with $\\boldsymbol{x}_i \\sim \\mathbb{P}(\\boldsymbol{\\theta}) ~~ \\forall i \\in 1\\dots m$<br>\n",
    "$\\quad\\quad$ elite $\\leftarrow $ { $m_{\\text{elite}}$ best samples } $\\quad$ (i.e. select best samples according to $f$)<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow $ fit $\\mathbb{P}(\\boldsymbol{\\theta})$ to the elite samples<br>\n",
    "\n",
    "**RETURN** $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1742285508784,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "O-D3naLi8U7_"
   },
   "source": [
    "def cem_uncorrelated(\n",
    "        env,\n",
    "        objective_function: Callable[[np.ndarray], float],\n",
    "        mean_array: np.ndarray,\n",
    "        var_array: np.ndarray,\n",
    "        max_iterations: int = 500,\n",
    "        sample_size: int = 50,\n",
    "        elite_frac: float = 0.2,\n",
    "        print_every: int = 10,\n",
    "        success_score: float = float(\"inf\"),\n",
    "        num_evals_for_stop: Optional[int] = None,\n",
    "        hist_dict: Optional[dict] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cross-entropy method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    objective_function : Callable[[np.ndarray], float]\n",
    "        The function to maximize.\n",
    "    mean_array : np.ndarray\n",
    "        The initial proposal distribution (mean vector).\n",
    "    var_array : np.ndarray\n",
    "        The initial proposal distribution (variance vector).\n",
    "    max_iterations : int, optional\n",
    "        Number of training iterations, by default 500.\n",
    "    sample_size : int, optional\n",
    "        Size of population at each iteration, by default 50.\n",
    "    elite_frac : float, optional\n",
    "        Rate of top performers to use in update with elite_frac ∈ ]0;1], by default 0.2.\n",
    "    print_every : int, optional\n",
    "        How often to print average score, by default 10.\n",
    "    success_score : float, optional\n",
    "        The score at which to stop the optimization, by default float(\"inf\").\n",
    "    num_evals_for_stop : Optional[int], optional\n",
    "        Number of evaluations for stopping criteria, by default None.\n",
    "    hist_dict : Optional[dict], optional\n",
    "        Dictionary to log the history, by default None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The optimized mean vector.\n",
    "    \"\"\"\n",
    "    assert 0.0 < elite_frac <= 1.0\n",
    "\n",
    "    n_elite = math.ceil(sample_size * elite_frac)\n",
    "\n",
    "    for iteration_index in range(0, max_iterations):\n",
    "\n",
    "        # SAMPLE A NEW POPULATION OF SOLUTIONS (X VECTORS) ####################\n",
    "\n",
    "        x_array = np.random.normal(mean_array, np.sqrt(var_array), size=(sample_size, len(mean_array)))\n",
    "\n",
    "        # EVALUATE SAMPLES AND EXTRACT THE BEST ONES (\"ELITE\") ################\n",
    "\n",
    "        score_array = np.array([objective_function(env, x) for x in x_array])\n",
    "\n",
    "        sorted_indices_array = np.argsort(\n",
    "            score_array)\n",
    "        elite_indices_array = sorted_indices_array[\n",
    "                              :n_elite]\n",
    "\n",
    "        elite_x_array = x_array[elite_indices_array]\n",
    "\n",
    "        # FIT THE NORMAL DISTRIBUTION ON THE ELITE POPULATION #################\n",
    "\n",
    "        mean_array = np.mean(elite_x_array, axis=0)\n",
    "        var_array = np.var(elite_x_array, axis=0)\n",
    "        score = np.min(score_array)\n",
    "\n",
    "        # PRINT STATUS ########################################################\n",
    "\n",
    "        if iteration_index % print_every == 0:\n",
    "            print(\"Iteration {}\\tScore {}\".format(iteration_index, score))\n",
    "\n",
    "        if hist_dict is not None:\n",
    "            hist_dict[iteration_index] = [score] + mean_array.tolist() + var_array.tolist()\n",
    "\n",
    "        # STOPPING CRITERIA ####################################################\n",
    "\n",
    "        if num_evals_for_stop is not None:\n",
    "            score = objective_function(mean_array)\n",
    "\n",
    "        # `num_evals_for_stop = None` may be used to fasten computations but it introduces bias...\n",
    "        if score <= success_score:\n",
    "            break\n",
    "\n",
    "    return mean_array"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1eT9sWAYSv2"
   },
   "source": [
    "### Training the agents (DQN v2 and REINFORCE) with CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc-pUlvsYSv2"
   },
   "source": [
    "### DQN v2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1742281179085,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "sWBWVw0QB1OW",
    "outputId": "ac5f3a67-5a71-4f75-8b07-0c6b89de938a"
   },
   "source": [
    "port = 3\n",
    "render_mode = 'human'\n",
    "env = kac.KingAndCourtesanEnv(port=port, render_mode=render_mode)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DMBokQtO8U7_"
   },
   "source": [
    "dqnv2_cem = QNetwork(env.observation_space.shape[0])\n",
    "#dqnv2_cem_adversary = QNetwork(env.observation_space.shape[0])\n",
    "\n",
    "objective_function = ObjectiveFunction(\n",
    "    env=env, policy=dqnv2_cem, adversaty_policy=dqnv2_cem, num_episodes=10, max_time_steps=300\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_params = len(dqnv2_cem.get_params())\n",
    "init_mean_array = np.zeros(num_params)\n",
    "init_var_array = np.ones(num_params) * 10.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "print(f\"Optimizing {num_params} parameters\")\n",
    "\n",
    "#init_mean_array = np.random.random(num_params)\n",
    "#init_var_array = np.ones(num_params) * 100.0\n",
    "\n",
    "optimized_policy_params_dqnv2_cem = cem_uncorrelated(\n",
    "    env,\n",
    "    objective_function=objective_function,\n",
    "    mean_array=init_mean_array,\n",
    "    var_array=init_var_array,\n",
    "    max_iterations=30,\n",
    "    sample_size=50,\n",
    "    elite_frac=0.1,\n",
    "    print_every=1,\n",
    "    success_score=-500,\n",
    "    num_evals_for_stop=None,\n",
    "    hist_dict=hist_dict,\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dqnv2_cem.set_params(optimized_policy_params_dqnv2_cem)\n",
    "torch.save(dqnv2_cem.state_dict(), MODELS_DIR / \"dqnv2_cem.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g7uSDqqO8U8A"
   },
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    hist_dict,\n",
    "    orient=\"index\",\n",
    "    columns=[\"score\", \"mu1\", \"mu2\", \"mu3\", \"mu4\", \"var1\", \"var2\", \"var3\", \"var4\"],\n",
    ")\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(20, 5))\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(PLOTS_DIR / \"dqnv2_cem_avg_reward_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sQMAsnNS8U8A"
   },
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(\n",
    "    title=\"Theta w.r.t training steps\", figsize=(20, 5)\n",
    ");\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"dqnv2_cem_params_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6bpiUKEhYSv2"
   },
   "source": [
    "ax = df[[\"var1\", \"var2\", \"var3\", \"var4\"]].plot(\n",
    "    logy=True, title=\"Variance w.r.t training steps\", figsize=(20, 5)\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"dqnv2_cem_var_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SGFHJzgUYSv2",
    "scrolled": true
   },
   "source": [
    "print(\"Optimized weights: \", optimized_policy_params_dqnv2_cem)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1BUZQpyYSv2"
   },
   "source": [
    "### REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reinforce_policy_nn_cem = PolicyNetwork(env.observation_space.shape[0])\n",
    "reinforce_policy_nn_cem = reinforce_policy_nn_cem.to(device)\n",
    "#reinforce_policy_nn_cem_adversary = PolicyNetwork(env.observation_space.shape[0])\n",
    "objective_function = ObjectiveFunction(\n",
    "    env=env, policy=reinforce_policy_nn_cem, adversary_policy=reinforce_policy_nn_cem, num_episodes=10,\n",
    "    max_time_steps=300\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "num_params = len(reinforce_policy_nn_cem.get_params())\n",
    "print(f\"Optimizing {num_params} parameters\")\n",
    "\n",
    "optimized_policy_params_reinforce_policy_nn_cem = cem_uncorrelated(\n",
    "    env,\n",
    "    objective_function=objective_function,\n",
    "    mean_array=init_mean_array,\n",
    "    var_array=init_var_array,\n",
    "    max_iterations=15,\n",
    "    sample_size=50,\n",
    "    elite_frac=0.1,\n",
    "    print_every=1,\n",
    "    success_score=-500,\n",
    "    num_evals_for_stop=None,\n",
    "    hist_dict=hist_dict,\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reinforce_policy_nn_cem.set_params(optimized_policy_params_reinforce_policy_nn_cem)\n",
    "torch.save(reinforce_policy_nn_cem.state_dict(), MODELS_DIR / \"reinforce_policy_nn_cem.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing against a random policy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_functions.test_policy_network_vs_random(env_port=3, num_episodes=1,\n",
    "                                             policy_network=reinforce_policy_nn_cem, max_steps=300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing against our Java ID Alpha Beta agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results_cem_reinforce = test_functions.test_policy_network_vs_alpha_beta(env_port=3, num_episodes=1,\n",
    "                                                                         policy_network=reinforce_policy_nn_cem,\n",
    "                                                                         agent_port=3, max_steps=300,\n",
    "                                                                         first_player_policy_network=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    hist_dict,\n",
    "    orient=\"index\",\n",
    "    columns=[\"score\", \"mu1\", \"mu2\", \"mu3\", \"mu4\", \"var1\", \"var2\", \"var3\", \"var4\"],\n",
    ")\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(20, 5))\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(PLOTS_DIR / \"reinforce_policy_nn_cem_avg_reward_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(\n",
    "    title=\"Theta w.r.t training steps\", figsize=(20, 5)\n",
    ");\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"reinforce_policy_nn_cem_params_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ax = df[[\"var1\", \"var2\", \"var3\", \"var4\"]].plot(\n",
    "    logy=True, title=\"Variance w.r.t training steps\", figsize=(20, 5)\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"reinforce_policy_nn_cem_var_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Optimized weights: \", optimized_policy_params_reinforce_policy_nn_cem)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFzybmTJYSv2"
   },
   "source": [
    "## 4.3 (1+1)-SA-ES optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7zIeXUQYSv3"
   },
   "source": [
    "`saes_1_1` function searchs the best $\\theta$ parameters with a (1+1)-SA-ES algorithm, using the objective function defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWwKxdkfYSv3"
   },
   "source": [
    "**(1+1)-SA-ES**\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad\\quad$ $f$: the objective function<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{x}$: initial solution<br>\n",
    "\n",
    "**Algorithm parameter**:<br>\n",
    "$\\quad\\quad$ $\\tau$: self-adaptation learning rate<br>\n",
    "\n",
    "**FOR EACH** generation<br>\n",
    "$\\quad\\quad$ 1. mutation of $\\sigma$ (current individual strategy) : $\\sigma' \\leftarrow \\sigma ~ e^{\\tau \\mathcal{N}(0,1)}$<br>\n",
    "$\\quad\\quad$ 2. mutation of $\\boldsymbol{x}$ (current solution) : $\\boldsymbol{x}' \\leftarrow \\boldsymbol{x} + \\sigma' ~ \\mathcal{N}(0,1)$<br>\n",
    "$\\quad\\quad$ 3. eval $f(\\boldsymbol{x}')$<br>\n",
    "$\\quad\\quad$ 4. survivor selection $\\boldsymbol{x} \\leftarrow \\boldsymbol{x}'$ and $\\sigma \\leftarrow \\sigma'$ if $f(\\boldsymbol{x}') \\leq f(\\boldsymbol{x})$<br>\n",
    "\n",
    "**RETURN** $\\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1742286894027,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "SV16-qO9YSv3"
   },
   "source": [
    "def saes_1_1(\n",
    "        env,\n",
    "        objective_function: Callable[[np.ndarray], float],\n",
    "        x_array: np.ndarray,\n",
    "        sigma_array: np.ndarray,\n",
    "        max_iterations: int = 500,\n",
    "        tau: Optional[float] = None,\n",
    "        print_every: int = 10,\n",
    "        success_score: float = float(\"inf\"),\n",
    "        num_evals_for_stop: Optional[int] = None,\n",
    "        hist_dict: Optional[dict] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    (1+1)-Self-Adaptive Evolution Strategy (SA-ES) optimization algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: KingAndCourtesanEnv\n",
    "        The environment in which to evaluate the policy.\n",
    "    objective_function : Callable[[np.ndarray], float]\n",
    "        The function to minimize.\n",
    "    x_array : np.ndarray\n",
    "        The initial solution vector.\n",
    "    sigma_array : np.ndarray\n",
    "        The initial strategy parameter vector (step sizes).\n",
    "    max_iterations : int, optional\n",
    "        The maximum number of iterations, by default 500.\n",
    "    tau : Optional[float], optional\n",
    "        The self-adaptation learning rate, by default None.\n",
    "    print_every : int, optional\n",
    "        How often to print the current score, by default 10.\n",
    "    success_score : float, optional\n",
    "        The score at which to stop the optimization, by default float(\"inf\").\n",
    "    num_evals_for_stop : Optional[int], optional\n",
    "        Number of evaluations for stopping criteria, by default None.\n",
    "    hist_dict : Optional[dict], optional\n",
    "        Dictionary to log the history, by default None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The optimized solution vector.\n",
    "    \"\"\"\n",
    "    # Number of dimension of the solution space\n",
    "    d = x_array.shape[0]\n",
    "\n",
    "    if tau is None:\n",
    "        # Self-adaptation learning rate\n",
    "        tau = 1.0 / (2.0 * d)\n",
    "\n",
    "    score = objective_function(env, x_array)\n",
    "\n",
    "    for iteration_index in range(0, max_iterations):\n",
    "        # 1. Mutation of sigma (current \"individual strategy\")\n",
    "        new_sigma_array = sigma_array * np.exp(tau * np.random.normal(0, 1, size=d))\n",
    "\n",
    "        # 2. Mutation of x (current solution)\n",
    "        new_x_array = x_array + new_sigma_array * np.random.normal(0, 1, size=d)\n",
    "\n",
    "        # 3. Eval f(x')\n",
    "        new_score = objective_function(env, new_x_array)\n",
    "\n",
    "        # 4. survivor selection (we follow the ES convention and do minimization)\n",
    "        if new_score <= score:  # You may try `new_score < score` for less exploration\n",
    "            score = new_score\n",
    "            x_array = new_x_array.copy()\n",
    "            sigma_array = new_sigma_array.copy()\n",
    "\n",
    "        # PRINT STATUS ########################################################\n",
    "\n",
    "        if iteration_index % print_every == 0:\n",
    "            print(\"Iteration {}\\tScore {}\".format(iteration_index, score))\n",
    "\n",
    "        if hist_dict is not None:\n",
    "            hist_dict[iteration_index] = [score] + x_array.tolist() + sigma_array.tolist()\n",
    "\n",
    "        # STOPPING CRITERIA ####################################################\n",
    "\n",
    "        if num_evals_for_stop is not None:\n",
    "            score = objective_function(env, x_array)\n",
    "\n",
    "        # `num_evals_for_stop = None` may be used to fasten computations but it introduces bias...\n",
    "        if score <= success_score:\n",
    "            break\n",
    "\n",
    "    return x_array"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWtuvGs3YSv3"
   },
   "source": [
    "### Training the agents (DQN v2 and REINFORCE) with (1+1)-SA-ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ksl14kmrYSv3"
   },
   "source": [
    "### DQN v2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gty7lF8KYSv3"
   },
   "source": [
    "dqnv2_saes_1_1 = QNetwork(env.observation_space.shape[0])\n",
    "dqnv2_saes_1_1_adversary = QNetwork(env.observation_space.shape[0])\n",
    "\n",
    "objective_function = ObjectiveFunction(\n",
    "    env=env, policy=dqnv2_saes_1_1, adversary_policy=dqnv2_saes_1_1, num_episodes=10, max_time_steps=300\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1742286904724,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "4ktyQDZMFWc7"
   },
   "source": [
    "initial_solution_array = np.random.randn(num_params) * np.sqrt(1.0 / num_params)\n",
    "initial_sigma_array = np.ones(num_params) * 10.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TkDZR3GZYSv3"
   },
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "num_params = len(dqnv2_saes_1_1.get_params())\n",
    "\n",
    "optimized_policy_params_dqnv2_saes_1_1 = saes_1_1(\n",
    "    objective_function=objective_function,\n",
    "    x_array=initial_solution_array,\n",
    "    sigma_array=initial_sigma_array,\n",
    "    tau=0.001,\n",
    "    max_iterations=30,\n",
    "    print_every=1,\n",
    "    success_score=-500,\n",
    "    num_evals_for_stop=None,\n",
    "    hist_dict=hist_dict,\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mCYQIlS7B1OY"
   },
   "source": [
    "dqnv2_saes_1_1.set_params(optimized_policy_params_dqnv2_cem)\n",
    "torch.save(dqnv2_saes_1_1.state_dict(), MODELS_DIR / \"dqnv2_saes_1_1.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iH1jvtlZYSv3"
   },
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    hist_dict,\n",
    "    orient=\"index\",\n",
    "    columns=[\n",
    "        \"score\",\n",
    "        \"mu1\",\n",
    "        \"mu2\",\n",
    "        \"mu3\",\n",
    "        \"mu4\",\n",
    "        \"sigma1\",\n",
    "        \"sigma2\",\n",
    "        \"sigma3\",\n",
    "        \"sigma4\",\n",
    "    ],\n",
    ")\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(30, 5))\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(PLOTS_DIR / \"dqnv2_saes_1_1_avg_reward_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JmN1Lee9YSv3"
   },
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(\n",
    "    title=\"Theta w.r.t training steps\", figsize=(30, 5)\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"dqnv2_saes_1_1_params_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kCIehZMRYSv3"
   },
   "source": [
    "ax = df[[\"sigma1\", \"sigma2\", \"sigma3\", \"sigma4\"]].plot(\n",
    "    logy=True, title=\"Sigma w.r.t training steps\", figsize=(30, 5)\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"dqnv2_saes_1_1_var_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pgKchDVFYSv3"
   },
   "source": [
    "print(\"Optimized weights: \", optimized_policy_params_dqnv2_saes_1_1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxgqbbQ5YSv3"
   },
   "source": [
    "### REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1742286920473,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "M_3YXonSYSv3"
   },
   "source": [
    "reinforce_policy_nn_saes_1_1 = PolicyNetwork(env.observation_space.shape[0])\n",
    "reinforce_policy_nn_saes_1_1 = reinforce_policy_nn_saes_1_1.to(device)\n",
    "reinforce_policy_nn_saes_1_1_adversary = PolicyNetwork(env.observation_space.shape[0])\n",
    "objective_function = ObjectiveFunction(\n",
    "    env=env, policy=reinforce_policy_nn_saes_1_1, adversary_policy=reinforce_policy_nn_saes_1_1,\n",
    "    num_episodes=10, max_time_steps=300\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73933,
     "status": "ok",
     "timestamp": 1742287035997,
     "user": {
      "displayName": "Arnauld LINVANI",
      "userId": "15767780697894810406"
     },
     "user_tz": -60
    },
    "id": "F2tTOxCFYSv3",
    "outputId": "2a079fa1-da8a-4f8d-8f91-5ebb7fdebb60"
   },
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "num_params = len(reinforce_policy_nn_saes_1_1.get_params())\n",
    "\n",
    "#initial_solution_array = np.random.random(num_params)\n",
    "#initial_sigma_array = np.ones(num_params) * 1.0\n",
    "\n",
    "optimized_policy_params_reinforce_policy_nn_saes_1_1 = saes_1_1(\n",
    "    env,\n",
    "    objective_function=objective_function,\n",
    "    x_array=initial_solution_array,\n",
    "    sigma_array=initial_sigma_array,\n",
    "    tau=0.001,\n",
    "    max_iterations=50,\n",
    "    print_every=1,\n",
    "    success_score=-500,\n",
    "    num_evals_for_stop=None,\n",
    "    hist_dict=hist_dict,\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reinforce_policy_nn_saes_1_1.set_params(optimized_policy_params_reinforce_policy_nn_saes_1_1)\n",
    "torch.save(reinforce_policy_nn_saes_1_1.state_dict(), MODELS_DIR / \"reinforce_policy_nn_saes_1_1.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing against a random policy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_functions.test_policy_network_vs_random(env_port=3, num_episodes=1,\n",
    "                                             policy_network=reinforce_policy_nn_saes_1_1, max_steps=300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing against our Java ID Alpha Beta agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results_saes_1_1_reinforce = test_functions.test_policy_network_vs_alpha_beta(env_port=3, num_episodes=1,\n",
    "                                                                              policy_network=reinforce_policy_nn_saes_1_1,\n",
    "                                                                              agent_port=3, max_steps=300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    hist_dict,\n",
    "    orient=\"index\",\n",
    "    columns=[\n",
    "        \"score\",\n",
    "        \"mu1\",\n",
    "        \"mu2\",\n",
    "        \"mu3\",\n",
    "        \"mu4\",\n",
    "        \"sigma1\",\n",
    "        \"sigma2\",\n",
    "        \"sigma3\",\n",
    "        \"sigma4\",\n",
    "    ],\n",
    ")\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(30, 5))\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(PLOTS_DIR / \"reinforce_policy_nn_saes_1_1_avg_reward_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6T4f1_-DYSv3"
   },
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(\n",
    "    title=\"Theta w.r.t training steps\", figsize=(30, 5)\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"reinforce_policy_nn_saes_1_1_params_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xZ9YUsRwYSv3"
   },
   "source": [
    "ax = df[[\"sigma1\", \"sigma2\", \"sigma3\", \"sigma4\"]].plot(\n",
    "    logy=True, title=\"Sigma w.r.t training steps\", figsize=(30, 5)\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"reinforce_policy_nn_saes_1_1_var_wrt_iterations.png\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Optimized weights: \", optimized_policy_params_reinforce_policy_nn_saes_1_1)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
